[
    {
        "id": "answer_sql_queries",
        "title": "Answer SQL Queries",
        "description": "Train a Qwen 2.5 model to excel at SQL query generation, understanding, and optimization across various database scenarios.",
        "notes": "# SQL Query Assistant with Qwen 2.5\n\n## Overview\nThis recipe fine-tunes a Qwen 2.5 model to become a specialized SQL query assistant.\n\n## Important Considerations\n- Comprehensive SQL knowledge base\n- Focus on query optimization\n- Covers various SQL dialects\n\n## Training Tips\n- Balance between simple and complex queries\n- Test query correctness\n- Validate across different database types\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Write efficient SQL queries\n- Explain query optimization\n- Handle complex database operations",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "dataset",
                "name": "mlx-community/wikisql"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "WikiSQL",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\":\"Wiki SQL\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"formatting_template\":\"{{text}} ;\",\"dataset_name\":\"mlx-community/wikisql\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"1\",\"learning_rate\":\"0.005\",\"num_train_epochs\":\"2\",\"max_steps\":\"-1\",\"lora_r\":\"32\",\"lora_alpha\":\"64\",\"lora_dropout\":\"0.1\",\"adaptor_name\":\"WikiSQL\",\"_tlab_recipe_datasets\":{\"name\":\"mlx-community/wikisql\",\"path\":\"mlx-community/wikisql\"},\"_tlab_recipe_models\":{\"name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"path\":\"Qwen/Qwen2.5-1.5B-Instruct\"}}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1683322499436-f4383dd59f5a?q=80&w=2071&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "answer_sql_queries_mlx",
        "title": "Answer SQL Queries (for MLX)",
        "description": "Fine-tune a Llama 3.2 1B model to understand and generate SQL queries, optimized for Apple Silicon using MLX framework.",
        "notes": "# SQL Query Assistant with MLX\n\n## Overview\nThis recipe adapts a Llama 3.2 1B model for SQL query generation and explanation, optimized for MLX.\n\n## Important Considerations\n- MLX optimization for Apple Silicon\n- Uses WikiSQL dataset for diverse query types\n- Focus on practical SQL scenarios\n\n## Training Tips\n- Monitor query correctness\n- Test across different SQL complexities\n- Validate query execution results\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Generate correct SQL queries\n- Explain query logic\n- Handle various database scenarios",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "meta-llama/Llama-3.2-1B"
            },
            {
                "type": "dataset",
                "name": "mlx-community/wikisql"
            },
            {
                "type": "plugin",
                "name": "mlx_lora_trainer"
            }
        ],
        "tasks": [
            {
                "name": "WikiSQL-MLX",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "mlx_lora_trainer",
                "config_json": "{\"template_name\":\"WikiSQL-MLX\",\"plugin_name\":\"mlx_lora_trainer\",\"model_name\":\"meta-llama/Llama-3.2-1B\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"Given the following description of an SQL table and its columns, provide the corresponding SQL to answer the question.\\n{{text}}\",\"dataset_name\":\"mlx-community/wikisql\",\"lora_layers\":\"8\",\"batch_size\":\"4\",\"learning_rate\":\"0.0001\",\"lora_rank\":\"8\",\"lora_alpha\":\"160\",\"iters\":\"200\",\"steps_per_report\":\"10\",\"steps_per_eval\":\"50\",\"save_every\":\"50\",\"adaptor_name\":\"ml-qa\",\"_tlab_recipe_datasets\":{\"name\":\"mlx-community/wikisql\",\"path\":\"mlx-community/wikisql\"},\"_tlab_recipe_models\":{\"name\":\"meta-llama/Llama-3.2-1B\",\"path\":\"meta-llama/Llama-3.2-1B\"}}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1683322499436-f4383dd59f5a?q=80&w=2071&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "convert_to_mlx",
        "title": "Convert a Model to the MLX Format",
        "description": "Transform your model into the MLX format for compatibility with various deployment environments.",
        "notes": "# MLX Model Conversion\n\n## Overview\nThis recipe demonstrates how to convert a model to the MLX format for optimized performance on Apple Silicon.\n\n## Important Considerations\n- MLX format is optimized for Apple Silicon\n- Conversion preserves model architecture and weights\n- Enables efficient inference on Mac devices\n\n## Conversion Tips\n- Verify model compatibility\n- Check memory requirements\n- Test inference after conversion\n\n## Expected Outcomes\nAfter conversion, you will have:\n- MLX-compatible model\n- Optimized performance on Apple Silicon\n- Reduced memory footprint",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "plugin",
                "name": "airllm_mlx_server"
            }
        ],
        "tasks": [
            {
                "name": "convert_to_mlx",
                "task_type": "EXPORT",
                "plugin": "airllm_mlx_server",
                "config_json": "{\"template_name\":\"MLXConversion\",\"plugin_name\":\"airllm_mlx_server\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"output_format\":\"mlx\"}"
            }
        ],
        "workflows": [
            {
                "name": "MLX_Conversion",
                "config": {
                    "nodes": [
                        {
                            "id": "node_convert",
                            "type": "EXPORT",
                            "task": "convert_to_mlx",
                            "name": "MLX Conversion Task",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1563203369-26f2e4a5ccf7?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "deploy_to_edge",
        "title": "Deploy a Model to the Edge",
        "description": "Seamlessly deploy your model to edge devices using deployment tools. Ensure low-latency and efficient performance.",
        "notes": "# Edge Deployment\n\n## Overview\nThis recipe demonstrates how to deploy a model to edge devices for efficient inference.\n\n## Important Considerations\n- Edge devices have limited resources\n- Optimization for target hardware\n- Balance between performance and size\n\n## Deployment Tips\n- Test on target hardware\n- Monitor resource usage\n- Optimize for specific use case\n\n## Expected Outcomes\nAfter deployment, you will have:\n- Edge-optimized model\n- Low-latency inference\n- Efficient resource usage",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
            },
            {
                "type": "plugin",
                "name": "airllm_mlx_server"
            },
            {
                "type": "plugin",
                "name": "eleuther-ai-lm-evaluation-harness-mlx"
            }
        ],
        "tasks": [
            {
                "name": "optimize_for_edge",
                "task_type": "EXPORT",
                "plugin": "airllm_mlx_server",
                "config_json": "{\"template_name\":\"EdgeOptimization\",\"plugin_name\":\"airllm_mlx_server\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"output_format\":\"mlx\",\"optimize_for_edge\":true}"
            },
            {
                "name": "evaluate_edge_model",
                "task_type": "EVAL",
                "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
                "config_json": "{\"template_name\":\"EvalEdge\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"mmlu\",\"limit\":\"0.5\",\"run_name\":\"EvalEdge\"}"
            }
        ],
        "workflows": [
            {
                "name": "Edge_Deployment",
                "config": {
                    "nodes": [
                        {
                            "id": "node_optimize",
                            "type": "EXPORT",
                            "task": "optimize_for_edge",
                            "name": "Edge Optimization Task",
                            "out": [
                                "node_eval"
                            ]
                        },
                        {
                            "id": "node_eval",
                            "type": "EVAL",
                            "task": "evaluate_edge_model",
                            "name": "Evaluation Task",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1667984390538-3dea7a3fe33d?q=80&w=1932&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "dialogue_summarizing",
        "title": "Dialogue Summarizing",
        "description": "Fine-tune a TinyLlama model to create concise, accurate summaries of conversations and dialogues. Perfect for chat logs, meeting transcripts, and customer service interactions.",
        "notes": "# Dialogue Summarization with TinyLlama\n\n## Overview\nThis recipe demonstrates how to fine-tune a TinyLlama model specifically for dialogue summarization using the SAMSum dataset.\n\n## Important Considerations\n- TinyLlama is optimized for efficiency while maintaining good performance\n- Uses LoRA for memory-efficient fine-tuning\n- Dataset contains diverse conversation styles and formats\n\n## Training Tips\n- Monitor the quality of generated summaries\n- Balance between brevity and information retention\n- Pay attention to maintaining conversation context\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Generate concise summaries of conversations\n- Maintain key points and context\n- Handle various dialogue formats and styles",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
            },
            {
                "type": "dataset",
                "name": "knkarthick/samsum"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "train_tinyllama_summarizer",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "llama_trainer",
                "formatting_template": "Instruction: Summarize the Following\nPrompt: {{dialogue}}\nGeneration: {{summary}}",
                "config_json": "{\"template_name\":\"DialogueSummarizing\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"Instruction: Summarize the Following\\nPrompt: {{dialogue}}\\nGeneration: {{summary}}\",\"dataset_name\":\"knkarthick/samsum\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate\":\"0.00005\",\"num_train_epochs\":\"1\",\"max_steps\":\"-1\",\"lora_r\":\"32\",\"lora_alpha\":\"64\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"Summarizer\",\"_tlab_recipe_datasets\":{\"name\":\"knkarthick/samsum\",\"path\":\"knkarthick/samsum\"},\"_tlab_recipe_models\":{\"name\":\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\"path\":\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"}}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1590650046871-92c887180603?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "evaluate_model",
        "title": "Evaluate a Model",
        "description": "Assess the performance of your model using Eleuther Labs AI Evaluation Harness. Gain insights into accuracy and reliability.",
        "notes": "# Model Evaluation\n\n## Overview\nThis recipe demonstrates how to evaluate a model's performance using the Eleuther AI Evaluation Harness.\n\n## Important Considerations\n- Uses standardized benchmarks for comparison\n- Evaluates multiple aspects of model performance\n- Provides detailed metrics and analysis\n\n## Evaluation Tips\n- Choose appropriate evaluation tasks\n- Consider using multiple benchmarks\n- Compare results with baseline models\n\n## Expected Outcomes\nThe evaluation will provide:\n- Detailed performance metrics\n- Task-specific scores\n- Comparative analysis with other models",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
            },
            {
                "type": "dataset",
                "name": "spencer/samsum_reformat"
            },
            {
                "type": "plugin",
                "name": "eleuther-ai-lm-evaluation-harness-mlx"
            }
        ],
        "tasks": [
            {
                "name": "evaluate_model_mmlu",
                "task_type": "EVAL",
                "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
                "config_json": "{\"template_name\":\"EvalMMLU\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"mmlu\",\"limit\":\"1.0\",\"run_name\":\"EvalMMLU\"}"
            },
            {
                "name": "evaluate_model_truthfulqa",
                "task_type": "EVAL",
                "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
                "config_json": "{\"template_name\":\"EvalTruthfulQA\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"truthfulqa\",\"limit\":\"1.0\",\"run_name\":\"EvalTruthfulQA\"}"
            }
        ],
        "workflows": [
            {
                "name": "Comprehensive_Evaluation",
                "config": {
                    "nodes": [
                        {
                            "id": "node_eval_mmlu",
                            "type": "EVAL",
                            "task": "evaluate_model_mmlu",
                            "name": "MMLU Evaluation",
                            "out": [
                                "node_eval_truthfulqa"
                            ]
                        },
                        {
                            "id": "node_eval_truthfulqa",
                            "type": "EVAL",
                            "task": "evaluate_model_truthfulqa",
                            "name": "TruthfulQA Evaluation",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1606326608606-aa0b62935f2b?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "eval-common-benchmarks-non-mlx",
        "title": "Evaluate a Model on Common Benchmarks (MLX)",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "description": "Performs evaluation on common benchmarks using the Eleuther AI LM Eval Harness. It evaluates the model `mlx-community/Llama-3.2-1B-Instruct-4bit` on tasks such as MMLU, Winogrande, HellaSwag, and PIQA.",
        "notes": "",
        "dependencies": [
            {
                "type": "model",
                "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
            },
            {
                "type": "plugin",
                "name": "common-eleuther-ai-lm-eval-harness-mlx"
            }
        ],
        "tasks": [
            {
                "name": "CleanMink",
                "task_type": "EVAL",
                "plugin": "common-eleuther-ai-lm-eval-harness-mlx",
                "config_json": "{\"template_name\":\"CleanMink\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"hellaswag,mmlu,piqa,winogrande\",\"limit\":\"1\",\"run_name\":\"CleanMink\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"CleanMink\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"hellaswag,mmlu,piqa,winogrande\",\"limit\":\"1\",\"run_name\":\"CleanMink\",\"predefined_tasks\":\"\"}}"
            }
        ],
        "workflows": [
            {
                "name": "eval-on-common-benchmarks",
                "config": {
                    "nodes": [
                        {
                            "type": "START",
                            "id": "99b97abd-82de-4745-b64a-6540801261c1",
                            "name": "START",
                            "out": [
                                "06334a95-01c4-4ece-82fc-9a107a4036e2"
                            ]
                        },
                        {
                            "name": "Eval on Harness Benchmarks",
                            "task": "CleanMink",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": -75,
                                    "y": 105
                                }
                            },
                            "id": "06334a95-01c4-4ece-82fc-9a107a4036e2",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "card_image": "https://images.unsplash.com/photo-1553268169-8232852a2377?q=80&w=1740&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "eval-common-benchmarks-non-mlx",
        "title": "Evaluate a Model on Common Benchmarks (Non-MLX)",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "description": "Performs evaluation on common benchmarks using the Eleuther AI LM Eval Harness. It evaluates the model `unsloth/Llama-3.2-1B-Instruct` on tasks such as MMLU, Winogrande, HellaSwag, and PIQA.",
        "notes": "",
        "dependencies": [
            {
                "type": "model",
                "name": "unsloth/Llama-3.2-1B-Instruct"
            },
            {
                "type": "plugin",
                "name": "common-eleuther-ai-lm-eval-harness"
            }
        ],
        "tasks": [
            {
                "name": "KindMoose",
                "task_type": "EVAL",
                "plugin": "common-eleuther-ai-lm-eval-harness",
                "config_json": "{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\"}}"
            }
        ],
        "workflows": [
            {
                "name": "evaluate-on-common-benchmarks",
                "config": {
                    "nodes": [
                        {
                            "type": "START",
                            "id": "bc9dc3a4-afba-4956-a55f-bd51e96da24f",
                            "name": "START",
                            "out": [
                                "009309fa-1ed5-42bc-be5d-b84e32772bf1"
                            ]
                        },
                        {
                            "name": "EVAL HARNESS",
                            "task": "KindMoose",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": -60,
                                    "y": 75
                                }
                            },
                            "id": "009309fa-1ed5-42bc-be5d-b84e32772bf1",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "card_image": "https://images.unsplash.com/photo-1589595427524-2ddaf2d43fc9?q=80&w=1744&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "fine_tune_small_mlx",
        "title": "Fine Tune a Small Language Model using MLX",
        "description": "Train a Llama 3.2 1B model to understand and answer questions about Touch Rugby rules using the MLX framework. Perfect for rule-based question answering.",
        "notes": "# MLX Fine-Tuning Notes\n\n## Overview\nThis recipe fine-tunes a Llama 3.2 1B model specifically for Touch Rugby rules using the MLX framework.\n\n## Important Considerations\n- MLX is optimized for Apple Silicon (M1/M2/M3 chips)\n- The dataset contains Touch Rugby rules in Q&A format\n- Model size is kept small (1B parameters) for efficient inference\n\n## Training Tips\n- Monitor loss curves carefully\n- Use appropriate LoRA rank (typically 8-64)\n- Validate on unseen rugby scenarios\n\n## Expected Outcomes\nAfter training, the model should be able to answer questions about:\n- Touch Rugby rules and regulations\n- Game procedures and scoring\n- Player positions and responsibilities",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
            },
            {
                "type": "plugin",
                "name": "mlx_lora_trainer"
            },
            {
                "type": "dataset",
                "name": "Trelis/touch-rugby-rules"
            },
            {
                "type": "plugin",
                "name": "common-eleuther-ai-lm-eval-harness-mlx"
            },
            {
                "type": "plugin",
                "name": "synthesizer_scratch"
            }
        ],
        "tasks": [
            {
                "name": "fine_tune_touch_rugby",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "mlx_lora_trainer",
                "formatting_template": "{{prompt}}\n{{completion}}",
                "config_json": "{\"template_name\":\"TouchRugby\",\"plugin_name\":\"mlx_lora_trainer\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"{{prompt}}\\n{{completion}}\",\"dataset_name\":\"Trelis/touch-rugby-rules\",\"lora_layers\":\"16\",\"batch_size\":\"8\",\"learning_rate\":\"0.0001\",\"lora_rank\":\"32\",\"lora_alpha\":\"128\",\"iters\":\"120\",\"steps_per_report\":\"10\",\"steps_per_eval\":\"20\",\"save_every\":\"10\",\"adaptor\":\"touch-rugby\"}"
            },
            {
                "name": "evaluate_touch_rugby",
                "task_type": "EVAL",
                "plugin": "common-eleuther-ai-lm-eval-harness-mlx",
                "config_json": "{\"template_name\":\"HandsomeBadger\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"mmlu\",\"limit\":\"0.5\",\"run_name\":\"HandsomeBadger\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"HandsomeBadger\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"mmlu\",\"limit\":\"0.5\",\"run_name\":\"HandsomeBadger\",\"predefined_tasks\":\"\"}}"
            },
            {
                "name": "generate_touch_rugby_examples",
                "task_type": "GENERATE",
                "plugin": "synthesizer_scratch",
                "config_json": "{\"template_name\":\"SparklingNarwhal\",\"plugin_name\":\"synthesizer_scratch\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"generation_model\":\"local\",\"num_goldens\":\"51\",\"generate_expected_output\":\"Yes\",\"scenario\":\"You are an expert in Touch Rugby rules and regulations. Generate diverse training examples that cover various aspects of the game.\",\"task\":\"Create question-answer pairs about Touch Rugby rules, focusing on game procedures, scoring rules, player positions, and common scenarios.\",\"input_format\":\"A specific question about Touch Rugby rules, formatted as: Question: [question text].\",\"expected_output_format\":\"A clear, accurate answer explaining the relevant Touch Rugby rule, formatted as: Answer: [detailed explanation]\",\"run_name\":\"SparklingNarwhal\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"SparklingNarwhal\",\"plugin_name\":\"synthesizer_scratch\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"generation_model\":\"local\",\"num_goldens\":\"51\",\"generate_expected_output\":\"Yes\",\"scenario\":\"You are an expert in Touch Rugby rules and regulations. Generate diverse training examples that cover various aspects of the game.\",\"task\":\"Create question-answer pairs about Touch Rugby rules, focusing on game procedures, scoring rules, player positions, and common scenarios.\",\"input_format\":\"A specific question about Touch Rugby rules, formatted as: Question: [question text].\",\"expected_output_format\":\"A clear, accurate answer explaining the relevant Touch Rugby rule, formatted as: Answer: [detailed explanation]\",\"run_name\":\"SparklingNarwhal\",\"generation_type\":\"scratch\"}}"
            }
        ],
        "workflows": [
            {
                "name": "Workflow_1",
                "config": {
                    "nodes": [
                        {
                            "id": "node_train",
                            "type": "TRAIN",
                            "task": "fine_tune_touch_rugby",
                            "name": "Training Task",
                            "out": [
                                "node_eval"
                            ]
                        },
                        {
                            "id": "node_eval",
                            "type": "EVAL",
                            "task": "evaluate_touch_rugby",
                            "name": "Evaluation Task",
                            "out": [
                                "node_generate"
                            ]
                        },
                        {
                            "id": "node_generate",
                            "type": "GENERATE",
                            "task": "generate_touch_rugby_examples",
                            "name": "Generation Task",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1558151507-c1aa3d917dbb?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "fine_tune_existing_model",
        "title": "Fine-tune an Existing Model",
        "description": "Adapt a pre-trained model to your specific needs using LoRA. Save time and resources by leveraging existing knowledge.",
        "notes": "# Fine-tuning with LoRA\n\n## Overview\nThis recipe demonstrates how to fine-tune a pre-trained model using Low-Rank Adaptation (LoRA) for efficient training.\n\n## Important Considerations\n- LoRA reduces memory requirements compared to full fine-tuning\n- Suitable for domain adaptation and task-specific training\n- Preserves base model knowledge while learning new tasks\n\n## Training Tips\n- Choose appropriate LoRA rank (typically 8-64)\n- Monitor training loss and validation metrics\n- Adjust learning rate and batch size based on task\n\n## Expected Outcomes\nAfter training, the model should:\n- Show improved performance on target domain\n- Maintain general language capabilities\n- Have smaller parameter footprint than full fine-tuning",
        "requiredMachineArchitecture": [
            "cuda"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            },
            {
                "type": "dataset",
                "name": "knkarthick/samsum"
            },
            {
                "type": "plugin",
                "name": "eleuther-ai-lm-evaluation-harness-mlx"
            }
        ],
        "tasks": [
            {
                "name": "finetune_model",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "llama_trainer",
                "formatting_template": "Instruction: Summarize the Following\nPrompt: {{dialogue}}\nGeneration: {{summary}}",
                "config_json": "{\"template_name\":\"DialogueSummarizing\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"formatting_template\":\"Instruction: Summarize the Following\\nPrompt: {{dialogue}}\\nGeneration: {{summary}}\",\"dataset_name\":\"knkarthick/samsum\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate\":\"0.00005\",\"num_train_epochs\":\"1\",\"max_steps\":\"-1\",\"lora_r\":\"32\",\"lora_alpha\":\"64\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"Summarizer\"}"
            },
            {
                "name": "evaluate_finetuned",
                "task_type": "EVAL",
                "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
                "config_json": "{\"template_name\":\"EvalFineTuned\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"tasks\":\"mmlu\",\"limit\":\"0.5\",\"run_name\":\"EvalFineTuned\"}"
            }
        ],
        "workflows": [
            {
                "name": "Finetune_and_Evaluate",
                "config": {
                    "nodes": [
                        {
                            "id": "node_finetune",
                            "type": "TRAIN",
                            "task": "finetune_model",
                            "name": "Fine-tuning Task",
                            "out": [
                                "node_eval"
                            ]
                        },
                        {
                            "id": "node_eval",
                            "type": "EVAL",
                            "task": "evaluate_finetuned",
                            "name": "Evaluation Task",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1561375996-8bbec3f2a481?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "ml_qa",
        "title": "Machine Learning Q&A",
        "description": "Train a Qwen 2.5 model to provide expert-level answers to machine learning questions, suitable for both beginners and advanced practitioners.",
        "notes": "# Machine Learning Q&A with Qwen 2.5\n\n## Overview\nThis recipe fine-tunes a Qwen 2.5 model to become a specialized machine learning assistant.\n\n## Important Considerations\n- Comprehensive ML knowledge coverage\n- Balanced between theoretical and practical knowledge\n- Suitable for various ML expertise levels\n\n## Training Tips\n- Focus on explanation clarity\n- Balance technical depth with accessibility\n- Validate answers across ML domains\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Provide detailed ML explanations\n- Answer implementation questions\n- Guide through ML concepts progressively",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "dataset",
                "name": "win-wang/Machine_Learning_QA_Collection"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "MachineLearningQnA",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\":\"MachineLearningQnA\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"formatting_template\":\"{{text}}\\n\",\"dataset_name\":\"win-wang/Machine_Learning_QA_Collection\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"1\",\"learning_rate\":\"0.00005\",\"num_train_epochs\":\"1\",\"max_steps\":\"-1\",\"lora_r\":\"16\",\"lora_alpha\":\"64\",\"lora_dropout\":\"0.1\",\"adaptor_name\":\"ML-QA\",\"_tlab_recipe_datasets\":{\"name\":\"win-wang/Machine_Learning_QA_Collection\",\"path\":\"win-wang/Machine_Learning_QA_Collection\"},\"_tlab_recipe_models\":{\"name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"path\":\"Qwen/Qwen2.5-1.5B-Instruct\"}}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1557562645-4eee56b29bc1?q=80&w=1935&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "ml_qa_mlx",
        "title": "Machine Learning Q&A (for MLX)",
        "description": "Fine-tune a Gemma 2 model to become an expert in answering machine learning questions, optimized for Apple Silicon using MLX framework.",
        "notes": "# Machine Learning Q&A with Gemma 2\n\n## Overview\nThis recipe adapts a Gemma 2 model to specialize in machine learning concepts and explanations, optimized for MLX.\n\n## Important Considerations\n- MLX optimization for Apple Silicon\n- Comprehensive ML Q&A dataset coverage\n- Efficient inference on Mac devices\n\n## Training Tips\n- Balance technical accuracy with clarity\n- Monitor explanation quality\n- Test across different ML topics\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Explain ML concepts clearly\n- Provide technical insights\n- Answer both basic and advanced ML questions",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "google/gemma-2-2b-it"
            },
            {
                "type": "dataset",
                "name": "win-wang/Machine_Learning_QA_Collection"
            },
            {
                "type": "plugin",
                "name": "mlx_lora_trainer"
            }
        ],
        "tasks": [
            {
                "name": "train_gemma_2_ml_qa",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "mlx_lora_trainer",
                "config_json": "{\"template_name\":\"MachineLearningQnA-MLX\",\"plugin_name\":\"mlx_lora_trainer\",\"model_name\":\"google/gemma-2-2b-it\",\"model_architecture\":\"Gemma2ForCausalLM\",\"formatting_template\":\"{{text}}\",\"dataset_name\":\"win-wang/Machine_Learning_QA_Collection\",\"lora_layers\":\"8\",\"batch_size\":\"4\",\"learning_rate\":\"0.0001\",\"lora_rank\":\"8\",\"lora_alpha\":\"160\",\"iters\":\"200\",\"steps_per_report\":\"10\",\"steps_per_eval\":\"50\",\"save_every\":\"50\",\"adaptor_name\":\"ml-qa\",\"_tlab_recipe_datasets\":{\"name\":\"Machine Learning QA Collection\",\"path\":\"win-wang/Machine_Learning_QA_Collection\"},\"_tlab_recipe_models\":{\"name\":\"google/gemma-2-2b-it\",\"path\":\"google/gemma-2-2b-it\"}}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1557562645-4eee56b29bc1?q=80&w=1935&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "python_code_completion",
        "title": "Python Code Completion",
        "description": "Train a SmolLM Base model to provide intelligent Python code completions, suggestions, and assistance. Ideal for developers looking for an efficient coding assistant.",
        "notes": "# Python Code Completion Training\n\n## Overview\nThis recipe fine-tunes a SmolLM Base model to become a specialized Python code completion assistant.\n\n## Important Considerations\n- SmolLM is designed for efficient inference\n- Dataset contains diverse Python coding examples\n- Model learns common Python patterns and best practices\n\n## Training Tips\n- Focus on code context understanding\n- Balance between common and specialized completions\n- Monitor completion accuracy and relevance\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Provide context-aware code completions\n- Suggest appropriate Python syntax\n- Complete common programming patterns",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "HuggingFaceTB/SmolLM2-135M"
            },
            {
                "type": "dataset",
                "name": "flytech/python-codes-25k"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "train_smollm_python_completion",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "llama_trainer",
                "formatting_template": "{{output}}",
                "config_json": "{\"template_name\":\"PythonCompletion\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"HuggingFaceTB/SmolLM2-135M\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"{{output}}\\n\",\"dataset_name\":\"flytech/python-codes-25k\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate\":\"0.0005\",\"num_train_epochs\":\"1\",\"max_steps\":\"-1\",\"lora_r\":\"64\",\"lora_alpha\":\"128\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"python\",\"_tlab_recipe_datasets\":{\"name\":\"flytech/python-codes-25k\",\"path\":\"flytech/python-codes-25k\"},\"_tlab_recipe_models\":{\"name\":\"HuggingFaceTB/SmolLM2-135M\",\"path\":\"HuggingFaceTB/SmolLM2-135M\"}}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?q=80&w=2069&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "quantize_model",
        "title": "Quantize a Model",
        "description": "Optimize your model for faster inference and reduced size using quantization tools.",
        "notes": "# Model Quantization\n\n## Overview\nThis recipe demonstrates how to quantize a model to reduce its size and improve inference speed while maintaining performance.\n\n## Important Considerations\n- Quantization reduces model precision\n- Trade-off between size/speed and accuracy\n- Different quantization methods available\n\n## Quantization Tips\n- Test different quantization levels\n- Validate performance after quantization\n- Consider hardware compatibility\n\n## Expected Outcomes\nAfter quantization, you will have:\n- Reduced model size\n- Faster inference speed\n- Minimal accuracy loss",
        "requiredMachineArchitecture": [
            "mlx",
            "cuda"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "plugin",
                "name": "llama_cpp_server"
            },
            {
                "type": "plugin",
                "name": "eleuther-ai-lm-evaluation-harness-mlx"
            }
        ],
        "tasks": [
            {
                "name": "quantize_model",
                "task_type": "EXPORT",
                "plugin": "llama_cpp_server",
                "config_json": "{\"template_name\":\"Quantization\",\"plugin_name\":\"llama_cpp_server\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"quantization_type\":\"q4_k_m\",\"n_gpu_layers\":\"auto\"}"
            },
            {
                "name": "evaluate_quantized",
                "task_type": "EVAL",
                "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
                "config_json": "{\"template_name\":\"EvalQuantized\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"tasks\":\"mmlu\",\"limit\":\"0.5\",\"run_name\":\"EvalQuantized\"}"
            }
        ],
        "workflows": [
            {
                "name": "Quantize_and_Evaluate",
                "config": {
                    "nodes": [
                        {
                            "id": "node_quantize",
                            "type": "EXPORT",
                            "task": "quantize_model",
                            "name": "Quantization Task",
                            "out": [
                                "node_eval"
                            ]
                        },
                        {
                            "id": "node_eval",
                            "type": "EVAL",
                            "task": "evaluate_quantized",
                            "name": "Evaluation Task",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "rag_pipeline",
        "title": "Retrieval Augmented Generation (RAG) Pipeline",
        "description": "Build and evaluate a complete custom Retrieval Augmented Generation (RAG) pipeline with a trained embedding model. RAG enhances language models by retrieving relevant information from external knowledge sources before generating responses.",
        "notes": "# Custom RAG Pipeline with Trained Embedding Model\n\n## Overview\nThis recipe builds a complete **Retrieval Augmented Generation (RAG) pipeline** that trains a custom embedding model specifically for your data and compares its performance against baseline embeddings. RAG enhances language models by retrieving relevant information from external knowledge sources before generating responses, dramatically improving accuracy and reducing hallucinations.\n\n## Steps to Follow\n\n### Step 1: Upload Your Documents\nBefore running any workflows, you need to provide the documents that will serve as your knowledge base:\n\n1. Navigate to the **Documents** tab in Transformer Lab\n2. Create a new folder called `rag`\n3. Upload all the documents you want to use as your knowledge base into this folder\n4. These documents will be used to generate Q&A pairs and train your custom embedding model\n\n### Step 2: Run Part 1 of the Workflow\nOnce your documents are uploaded:\n\n1. Go to the **Workflows** page\n2. Find and run **rag_pipeline_part_1**\n3. This will:\n   - Generate a Q&A dataset from your documents\n   - Train a custom embedding model on your data\n   - Generate baseline outputs using the default embedding model\n\nWait for Part 1 to complete before proceeding to Step 3.\n\n### Step 3: Switch to Your Trained Embedding Model\nAfter Part 1 completes, you need to configure the system to use your newly trained embedding model:\n\n1. Go to the **Foundation** tab\n2. In the **Embedding Models** field, change the selection to any model beginning with:\n   `TrainedEmbeddingModel` followed by numbers\n3. This switches the system to use your custom-trained embedding model for retrieval\n\n### Step 4: Run Part 2 of the Workflow\nWith your trained embedding model now active:\n\n1. Return to the **Workflows** page\n2. Find and run **rag_pipeline_part_2**\n3. This will:\n   - Generate outputs using your trained embedding model\n   - Evaluate the performance of both the baseline and trained models\n   - Provide comparison metrics showing the improvement\n\n## Understanding RAG Benefits\nThis recipe demonstrates the key advantages of RAG:\n- **Reduced Hallucinations**: Models access real information instead of relying on training data\n- **Domain Expertise**: Custom embeddings understand your specific content better\n- **Up-to-date Information**: Retrieval allows access to current data not in training sets\n- **Measurable Improvement**: Direct comparison shows quantitative benefits\n\n## How to Use Your Trained Embedding Model\nOnce both workflows are complete, you can use your custom embedding model in any RAG application by keeping the **Foundation** tab configured with your `TrainedEmbeddingModel` selection. Your RAG queries will now benefit from domain-specific embeddings that understand your content better than generic models.\n\n## Important Notes\n- This recipe works on **Apple Silicon (MLX), CUDA, and AMD** architectures\n- The two-part workflow structure allows you to inspect results after training and before evaluation\n- **Automated evaluation** provides objective metrics comparing base vs. trained performance using Contextual Precision and Answer Relevancy\n- Uses the **Qwen 2.5-1.5B-Instruct** model for both generation and evaluation\n\n## Expected Outcomes\nAfter completing both workflows, you'll have:\n- A custom embedding model trained specifically for your domain\n- Quantitative evidence of improved RAG performance with custom embeddings\n- A complete RAG system ready for production use\n- Understanding of how domain-specific training enhances retrieval quality\n- Baseline metrics to measure future improvements",
        "requiredMachineArchitecture": [
            "mlx",
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "dataset",
                "name": "generateqnapairs"
            },
            {
                "type": "plugin",
                "name": "embedding_model_trainer"
            },
            {
                "type": "plugin",
                "name": "deepeval_llm_judge"
            },
            {
                "type": "plugin",
                "name": "generate_rag_outputs"
            },
            {
                "type": "plugin",
                "name": "synthetic_dataset_rag"
            },
            {
                "type": "plugin",
                "name": "llamaindex_simple_document_search"
            }
        ],
        "tasks": [
            {
                "name": "TrainEmbeddingModel",
                "task_type": "TRAIN",
                "plugin": "embedding_model_trainer",
                "config_json": "{\"template_name\":\"TrainEmbeddingModel\",\"plugin_name\":\"embedding_model_trainer\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"foundation_model_file_path\":\"\",\"embedding_model\":\"BAAI/bge-base-en-v1.5\",\"embedding_model_architecture\":\"BertModel\",\"embedding_model_file_path\":\"\",\"dataset_name\":\"generateqnapairs\",\"dataset_type\":\"single sentences\",\"loss_function\":\"DenoisingAutoEncoderLoss\",\"loss_modifier_name\":\"None\",\"text_column_name\":\"context\",\"num_train_epochs\":\"3\",\"batch_size\":\"16\",\"learning_rate\":\"0.00002\",\"warmup_ratio\":\"0.1\",\"max_samples\":\"-1\",\"type\":\"embedding\",\"run_sweeps\":false}"
            },
            {
                "name": "TrainedModelEval",
                "task_type": "EVAL",
                "plugin": "deepeval_llm_judge",
                "config_json": "{\"template_name\":\"TrainedModelEval\",\"plugin_name\":\"deepeval_llm_judge\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"generation_model\":\"local\",\"predefined_tasks\":\"Contextual Precision,Answer Relevancy\",\"tasks\":\"[]\",\"limit\":\"1\",\"dataset_split\":\"train\",\"dataset_name\":\"trainedembeddinganswers\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"TrainedModelEval\",\"script_parameters\":{\"template_name\":\"TrainedModelEval\",\"plugin_name\":\"deepeval_llm_judge\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"generation_model\":\"local\",\"predefined_tasks\":\"Contextual Precision,Answer Relevancy\",\"tasks\":\"[]\",\"limit\":\"1\",\"dataset_split\":\"train\",\"dataset_name\":\"peacefulchinchilla_2879\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"TrainedModelEval\"}}"
            },
            {
                "name": "BaseModelEval",
                "task_type": "EVAL",
                "plugin": "deepeval_llm_judge",
                "config_json": "{\"template_name\":\"BaseModelEval\",\"plugin_name\":\"deepeval_llm_judge\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"generation_model\":\"local\",\"predefined_tasks\":\"Contextual Precision,Answer Relevancy\",\"tasks\":\"[]\",\"limit\":\"1\",\"dataset_split\":\"train\",\"dataset_name\":\"baseembeddinganswers\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelEval\",\"script_parameters\":{\"template_name\":\"BaseModelEval\",\"plugin_name\":\"deepeval_llm_judge\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"generation_model\":\"local\",\"predefined_tasks\":\"Contextual Precision,Answer Relevancy\",\"tasks\":\"[]\",\"limit\":\"1\",\"dataset_split\":\"train\",\"dataset_name\":\"radiantdonkey_2877\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelEval\"}}"
            },
            {
                "name": "TrainedEmbeddingAnswers",
                "task_type": "GENERATE",
                "plugin": "generate_rag_outputs",
                "config_json": "{\"template_name\":\"TrainedEmbeddingAnswers\",\"plugin_name\":\"generate_rag_outputs\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"input_field\":\"input\",\"response_mode\":\"compact\",\"number_of_search_results\":\"2\",\"temperature\":\"1\",\"context_window\":\"4096\",\"num_output\":\"256\",\"chunk_size\":\"512\",\"chunk_overlap\":\"50\",\"reranker_model\":\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\"reranker_top_n\":\"20\",\"output_dataset_name\":\"generated_dataset\",\"dataset_name\":\"generateqnapairs\",\"_dataset_display_message\":\"This plugin requires a dataset to run. Please upload a dataset to continue.\",\"run_name\":\"TrainedEmbeddingAnswers\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"TrainedEmbeddingAnswers\",\"plugin_name\":\"generate_rag_outputs\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"input_field\":\"input\",\"response_mode\":\"compact\",\"number_of_search_results\":\"2\",\"temperature\":\"1\",\"context_window\":\"4096\",\"num_output\":\"256\",\"chunk_size\":\"512\",\"chunk_overlap\":\"50\",\"reranker_model\":\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\"reranker_top_n\":\"20\",\"output_dataset_name\":\"generated_dataset\",\"dataset_name\":\"generateqnapairs\",\"_dataset_display_message\":\"This plugin requires a dataset to run. Please upload a dataset to continue.\",\"run_name\":\"TrainedEmbeddingAnswers\",\"generation_type\":\"scratch\"}}"
            },
            {
                "name": "BaseEmbeddingAnswers",
                "task_type": "GENERATE",
                "plugin": "generate_rag_outputs",
                "config_json": "{\"template_name\":\"BaseEmbeddingAnswers\",\"plugin_name\":\"generate_rag_outputs\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"input_field\":\"input\",\"response_mode\":\"compact\",\"number_of_search_results\":\"2\",\"temperature\":\"1\",\"context_window\":\"4096\",\"num_output\":\"256\",\"chunk_size\":\"512\",\"chunk_overlap\":\"50\",\"reranker_model\":\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\"reranker_top_n\":\"20\",\"output_dataset_name\":\"generated_dataset\",\"dataset_name\":\"generateqnapairs\",\"_dataset_display_message\":\"This plugin requires a dataset to run. Please upload a dataset to continue.\",\"run_name\":\"BaseEmbeddingAnswers\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"BaseEmbeddingAnswers\",\"plugin_name\":\"generate_rag_outputs\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"input_field\":\"input\",\"response_mode\":\"compact\",\"number_of_search_results\":\"2\",\"temperature\":\"1\",\"context_window\":\"4096\",\"num_output\":\"256\",\"chunk_size\":\"512\",\"chunk_overlap\":\"50\",\"reranker_model\":\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\"reranker_top_n\":\"20\",\"output_dataset_name\":\"generated_dataset\",\"dataset_name\":\"generateqnapairs\",\"_dataset_display_message\":\"This plugin requires a dataset to run. Please upload a dataset to continue.\",\"run_name\":\"BaseEmbeddingAnswers\",\"generation_type\":\"scratch\"}}"
            },
            {
                "name": "GenerateQnAPairs",
                "task_type": "GENERATE",
                "plugin": "synthetic_dataset_rag",
                "config_json": "{\"template_name\":\"GenerateQnAPairs\",\"plugin_name\":\"synthetic_dataset_rag\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"generation_model\":\"local\",\"chunk_size\":\"256\",\"chunk_overlap\":\"200\",\"n_generations\":\"200\",\"output_dataset_name\":\"GenerateQnAPairs\",\"docs\":\"rag\",\"run_name\":\"GenerateQnAPairs\",\"generation_type\":\"docs\",\"script_parameters\":{\"template_name\":\"GenerateQnAPairs\",\"plugin_name\":\"synthetic_dataset_rag\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"generation_model\":\"local\",\"chunk_size\":\"256\",\"chunk_overlap\":\"200\",\"n_generations\":\"200\",\"output_dataset_name\":\"GenerateQnAPairs\",\"docs\":\"rag\",\"run_name\":\"GenerateQnAPairs\",\"generation_type\":\"docs\"}}"
            }
        ],
        "workflows": [
            {
                "name": "rag_pipeline_part_1",
                "config": {
                    "nodes": [
                        {
                            "type": "START",
                            "id": "b914cda4-8363-43ec-9154-8b9bf6d251a3",
                            "name": "START",
                            "out": [
                                "04e138e4-b1da-494d-b43d-77b69074a4d6"
                            ]
                        },
                        {
                            "name": "Generate_QnA_Dataset",
                            "task": "GenerateQnAPairs",
                            "type": "GENERATE",
                            "metadata": {
                                "position": {
                                    "x": -15,
                                    "y": 90
                                }
                            },
                            "id": "04e138e4-b1da-494d-b43d-77b69074a4d6",
                            "out": [
                                "1796289f-6cae-449b-a4ad-e318825cd9f1"
                            ]
                        },
                        {
                            "name": "Train_Embedding_Model",
                            "task": "TrainEmbeddingModel",
                            "type": "TRAIN",
                            "metadata": {
                                "position": {
                                    "x": -15,
                                    "y": 210
                                }
                            },
                            "id": "1796289f-6cae-449b-a4ad-e318825cd9f1",
                            "out": [
                                "ec414b7c-6b52-479c-a695-66619acf872d"
                            ]
                        },
                        {
                            "name": "Base_Model_Outputs",
                            "task": "BaseEmbeddingAnswers",
                            "type": "GENERATE",
                            "metadata": {
                                "position": {
                                    "x": 0,
                                    "y": 330
                                }
                            },
                            "id": "ec414b7c-6b52-479c-a695-66619acf872d",
                            "out": []
                        }
                    ]
                }
            },
            {
                "name": "rag_pipeline_part_2",
                "config": {
                    "nodes": [
                        {
                            "type": "START",
                            "id": "f1a2b3c4-d5e6-f7g8-h9i0-j1k2l3m4n5o6",
                            "name": "START",
                            "out": [
                                "a1b2c3d4-e5f6-g7h8-i9j0-k1l2m3n4o5p6"
                            ]
                        },
                        {
                            "name": "Trained_Model_Outputs",
                            "task": "TrainedEmbeddingAnswers",
                            "type": "GENERATE",
                            "metadata": {
                                "position": {
                                    "x": 0,
                                    "y": 90
                                }
                            },
                            "id": "a1b2c3d4-e5f6-g7h8-i9j0-k1l2m3n4o5p6",
                            "out": [
                                "b2c3d4e5-f6g7-h8i9-j0k1-l2m3n4o5p6q7"
                            ]
                        },
                        {
                            "name": "Base_Model_Eval",
                            "task": "BaseModelEval",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": 0,
                                    "y": 210
                                }
                            },
                            "id": "b2c3d4e5-f6g7-h8i9-j0k1-l2m3n4o5p6q7",
                            "out": [
                                "c3d4e5f6-g7h8-i9j0-k1l2-m3n4o5p6q7r8"
                            ]
                        },
                        {
                            "name": "Trained_Model_Eval",
                            "task": "TrainedModelEval",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": 0,
                                    "y": 330
                                }
                            },
                            "id": "c3d4e5f6-g7h8-i9j0-k1l2-m3n4o5p6q7r8",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/flagged/photo-1558963675-94dc9c4a66a9?q=80&w=686&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "run_on_edge",
        "title": "Run a Model on the Edge",
        "description": "Execute your model on edge devices using runtime environment. Achieve real-time performance and low power consumption.",
        "notes": "# Edge Inference\n\n## Overview\nThis recipe demonstrates how to run a model efficiently on edge devices.\n\n## Important Considerations\n- Resource constraints on edge devices\n- Real-time performance requirements\n- Power consumption optimization\n\n## Runtime Tips\n- Monitor inference latency\n- Optimize batch size for throughput\n- Balance CPU and GPU usage\n\n## Expected Outcomes\nAfter setup, you will have:\n- Efficient model inference\n- Real-time performance\n- Optimized resource usage",
        "requiredMachineArchitecture": [
            "cuda"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "plugin",
                "name": "fastchat_server"
            },
            {
                "type": "plugin",
                "name": "eleuther-ai-lm-evaluation-harness-mlx"
            }
        ],
        "tasks": [
            {
                "name": "setup_edge_inference",
                "task_type": "LOADER",
                "plugin": "fastchat_server",
                "config_json": "{\"template_name\":\"EdgeInference\",\"plugin_name\":\"fastchat_server\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"load_compressed\":\"4-bit\",\"model_dtype\":\"float16\"}"
            },
            {
                "name": "evaluate_inference",
                "task_type": "EVAL",
                "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
                "config_json": "{\"template_name\":\"EvalInference\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"tasks\":\"mmlu\",\"limit\":\"0.5\",\"run_name\":\"EvalInference\"}"
            }
        ],
        "workflows": [
            {
                "name": "Edge_Inference",
                "config": {
                    "nodes": [
                        {
                            "id": "node_setup",
                            "type": "LOADER",
                            "task": "setup_edge_inference",
                            "name": "Edge Setup Task",
                            "out": [
                                "node_eval"
                            ]
                        },
                        {
                            "id": "node_eval",
                            "type": "EVAL",
                            "task": "evaluate_inference",
                            "name": "Evaluation Task",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1667984550708-a6beba23cb4c?q=80&w=1932&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "train_from_scratch",
        "title": "Train a Model From Scratch",
        "description": "Build a new machine learning model from the ground up using MLX, optimized for Apple Silicon. Ideal for custom use cases and datasets.",
        "notes": "# Training a Model From Scratch\n\n## Overview\nThis recipe demonstrates how to train a new machine learning model from scratch using MLX framework, optimized for Apple Silicon.\n\n## Important Considerations\n- Optimized for Apple Silicon (M1/M2/M3)\n- Uses MLX for efficient training\n- Dataset is formatted for instruction following\n\n## Training Tips\n- Monitor loss curves carefully\n- Adjust batch size based on available memory\n- Use appropriate learning rate schedule\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Follow basic instructions\n- Generate coherent responses\n- Demonstrate basic language understanding",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
            },
            {
                "type": "plugin",
                "name": "mlx_lora_trainer"
            },
            {
                "type": "dataset",
                "name": "spencer/samsum_reformat"
            },
            {
                "type": "plugin",
                "name": "eleuther-ai-lm-evaluation-harness-mlx"
            }
        ],
        "tasks": [
            {
                "name": "train_from_scratch",
                "task_type": "TRAIN",
                "type": "Training",
                "plugin": "mlx_lora_trainer",
                "config_json": "{\"template_name\":\"TrainFromScratch\",\"plugin_name\":\"mlx_lora_trainer\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"{{text}}\",\"lora_layers\":\"16\",\"batch_size\":\"8\",\"learning_rate\":\"0.0001\",\"lora_rank\":\"32\",\"lora_alpha\":\"128\",\"iters\":\"120\",\"steps_per_report\":\"10\",\"steps_per_eval\":\"20\",\"save_every\":\"10\",\"adaptor\":\"from_scratch\"}"
            },
            {
                "name": "evaluate_model",
                "task_type": "EVAL",
                "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
                "config_json": "{\"template_name\":\"EvalFromScratch\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"mmlu\",\"limit\":\"0.5\",\"run_name\":\"EvalFromScratch\"}"
            }
        ],
        "workflows": [
            {
                "name": "Train_and_Evaluate",
                "config": {
                    "nodes": [
                        {
                            "id": "node_train",
                            "type": "TRAIN",
                            "task": "train_from_scratch",
                            "name": "Training Task",
                            "out": [
                                "node_eval"
                            ]
                        },
                        {
                            "id": "node_eval",
                            "type": "EVAL",
                            "task": "evaluate_model",
                            "name": "Evaluation Task",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://plus.unsplash.com/premium_photo-1682142051662-eda5ad640633?q=80&w=2071&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "conversational_intelligence",
        "title": "Train a Model to be Conversationally Intelligent",
        "description": "Enhance a SmolLM model with advanced conversational abilities and structured response formatting using XML tags, ideal for creating a sophisticated dialogue agent.",
        "notes": "# Conversational Intelligence Training\n\n## Overview\nThis recipe develops a SmolLM model into a sophisticated conversational agent using structured dialogue formats.\n\n## Important Considerations\n- XML-based response structuring\n- Focus on natural dialogue flow\n- Balanced conversation handling\n\n## Training Tips\n- Monitor response coherence\n- Validate XML format consistency\n- Test conversation flow\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Maintain structured conversations\n- Generate well-formatted responses\n- Handle diverse dialogue scenarios",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "HuggingFaceTB/SmolLM2-135M"
            },
            {
                "type": "dataset",
                "name": "nickrosh/Evol-Instruct-Code-80k-v1"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "InstructTuning",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\":\"InstructTuning\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"HuggingFaceTB/SmolLM2-135M\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"<User>\\n{{instruction}}\\n</User>\\n<Assistant>\\n{{output}}\\n</Assistant>\\n\",\"dataset_name\":\"nickrosh/Evol-Instruct-Code-80k-v1\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate\":\"0.00003\",\"num_train_epochs\":\"1\",\"max_steps\":\"-1\",\"lora_r\":\"4\",\"lora_alpha\":\"16\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"instruct\",\"_tlab_recipe_datasets\":{\"name\":\"nickrosh/Evol-Instruct-Code-80k-v1\",\"path\":\"nickrosh/Evol-Instruct-Code-80k-v1\"},\"_tlab_recipe_models\":{\"name\":\"HuggingFaceTB/SmolLM2-135M\",\"path\":\"HuggingFaceTB/SmolLM2-135M\"}}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1573497620053-ea5300f94f21?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "pirate_speech",
        "title": "Train a Model to Speak like a Pirate",
        "description": "Transform a SmolLM model into a charismatic pirate conversationalist, perfect for creating engaging and entertaining interactions with a nautical twist.",
        "notes": "# Pirate Speech Transformation\n\n## Overview\nThis recipe transforms a SmolLM model into an engaging pirate-speaking assistant using specialized dialogue data.\n\n## Important Considerations\n- Maintains coherent pirate-style speech\n- Balances authenticity with understandability\n- Preserves helpful responses in pirate style\n\n## Training Tips\n- Monitor consistency of pirate speech\n- Balance entertainment with usefulness\n- Maintain appropriate language level\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Respond in consistent pirate dialect\n- Maintain helpful information delivery\n- Create engaging pirate-themed interactions",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "HuggingFaceTB/SmolLM-135M-Instruct"
            },
            {
                "type": "dataset",
                "name": "Peyton3995/dolly-15k-mistral-pirate"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "PirateSpeech",
                "task_type": "TRAIN",
                "type": "LoRA",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\":\"PirateSpeech\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"HuggingFaceTB/SmolLM-135M-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"<instruction>\\n{{instruction}}\\n</instruction>\\n<response>\\n{{response}}\\n</response>\",\"dataset_name\":\"Peyton3995/dolly-15k-mistral-pirate\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate_schedule\":\"cosine\",\"learning_rate\":\"0.01\",\"num_train_epochs\":\"1\",\"max_steps\":\"-1\",\"lora_r\":\"64\",\"lora_alpha\":\"128\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"Pirate_Speech\",\"_tlab_recipe_datasets\":{\"name\":\"Peyton3995/dolly-15k-mistral-pirate\",\"path\":\"Peyton3995/dolly-15k-mistral-pirate\"},\"_tlab_recipe_models\":{\"name\":\"HuggingFaceTB/SmolLM-135M-Instruct\",\"path\":\"HuggingFaceTB/SmolLM-135M-Instruct\"}}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1652447275071-4bf852aebdc5?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    }
]