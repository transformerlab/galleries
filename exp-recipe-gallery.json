[
    {
        "id": "answer_sql_queries",
        "title": "Answer SQL Queries",
        "description": "Train a Qwen 2.5 model to excel at SQL query generation, understanding, and optimization across various database scenarios.",
        "notes": "# SQL Query Assistant with Qwen 2.5\n\n<-- To run this recipe, go to **Train** in the sidebar and click on Queue\n\n## Overview\nThis recipe fine-tunes a Qwen 2.5 model to become a specialized SQL query assistant using the wikiSQL dataset.\n\n## Expected Outcome\nAfter training, the model should be able to:\n- Write efficient SQL queries\n- Explain query optimization\n- Handle complex database operations",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "dataset",
                "name": "mlx-community/wikisql"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "WikiSQL",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"Wiki SQL\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"model_architecture\": \"Qwen2ForCausalLM\", \"formatting_template\": \"{{text}} ;\", \"dataset_name\": \"mlx-community/wikisql\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"1\", \"learning_rate\": \"0.005\", \"num_train_epochs\": \"2\", \"max_steps\": \"-1\", \"lora_r\": \"32\", \"lora_alpha\": \"64\", \"lora_dropout\": \"0.1\", \"adaptor_name\": \"WikiSQL\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"mlx-community/wikisql\\\", \\\"path\\\": \\\"mlx-community/wikisql\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\", \\\"path\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\"}\"}",
                "inputs_json": "{\"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"model_architecture\": \"Qwen2ForCausalLM\", \"dataset_name\": \"mlx-community/wikisql\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1683322499436-f4383dd59f5a?q=80&w=2071&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "quantize_model_cpu_gguf",
        "title": "Quantize a Model on your CPU device",
        "description": "Optimize your model for faster inference and reduced size using quantization tools.",
        "notes": "# Model Quantization\n\nThis recipe quantizes a model to reduce its size and improve inference speed while maintaining performance.\n\n## How to Use\nTo get a quantized model, simply go to the **Export** tab and run the `ExportGGUF` task. Once it is complete, you can go to the Model Zoo -> Generated to see your newly generated model.\n\n## What it does\n- Converts your model to GGUF format with q8_0 quantization\n- Reduces model size for faster loading\n- Maintains good performance with minimal accuracy loss",
        "requiredMachineArchitecture": [
            "cpu"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "unsloth/Llama-3.2-1B-Instruct"
            },
            {
                "type": "plugin",
                "name": "gguf_exporter"
            }
        ],
        "tasks": [
            {
                "name": "ExportGGUF",
                "task_type": "EXPORT",
                "plugin": "gguf_exporter",
                "config_json": "{\"plugin_name\":\"gguf_exporter\",\"input_model_id\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_path\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_architecture\":\"LlamaForCausalLM\",\"output_model_id\":\"Llama-3.2-1B-Instruct-1752789244-q8_0.gguf\",\"output_model_architecture\":\"GGUF\",\"output_model_name\":\"Llama-3.2-1B-Instruct - GGUF - q8_0\",\"output_model_path\":\"/models/Llama-3.2-1B-Instruct-1752789244-q8_0.gguf\",\"output_filename\":\"Llama-3.2-1B-Instruct-1752789244-q8_0.gguf\",\"script_directory\":\"/plugins/gguf_exporter\",\"params\":{\"outtype\":\"q8_0\"},\"run_name\":\"ExportGGUF\"}",
                "inputs_json": "{\"input_model_id\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_path\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_architecture\":\"LlamaForCausalLM\",\"plugin_name\":\"gguf_exporter\",\"plugin_architecture\":\"GGUF\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://recipes.transformerlab.net/quantization.png"
    },
    {
        "id": "dialogue_summarizing",
        "title": "Dialogue Summarizing",
        "description": "Fine-tune a TinyLlama model to create concise, accurate summaries of conversations and dialogues. Perfect for chat logs, meeting transcripts, and customer service interactions.",
        "notes": "# Dialogue Summarization with TinyLlama\n\n<-- To run this recipe, go to **Train** in the sidebar and click on Queue\n\n## Overview\nThis recipe fine-tunes a TinyLlama model to create concise, accurate summaries of conversations and dialogues using the popular Samsum dataset.\n\n## Expected Outcome\nAfter training, the model should be able to:\n- Generate concise summaries of conversations\n- Maintain key points and context\n- Handle various dialogue formats and styles",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
            },
            {
                "type": "dataset",
                "name": "knkarthick/samsum"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "train_tinyllama_summarizer",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"DialogueSummarizing\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"model_architecture\": \"LlamaForCausalLM\", \"formatting_template\": \"Instruction: Summarize the Following\\nPrompt: {{dialogue}}\\nGeneration: {{summary}}\", \"dataset_name\": \"knkarthick/samsum\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"4\", \"learning_rate\": \"0.00005\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"32\", \"lora_alpha\": \"64\", \"lora_dropout\": \"0.05\", \"adaptor_name\": \"Summarizer\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"knkarthick/samsum\\\", \\\"path\\\": \\\"knkarthick/samsum\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\\\", \\\"path\\\": \\\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\\\"}\"}",
                "inputs_json": "{\"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"model_architecture\": \"LlamaForCausalLM\", \"dataset_name\": \"knkarthick/samsum\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1590650046871-92c887180603?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "diffusion_train",
        "title": "Train a Diffusion LoRA",
        "description": "Train a LoRA adapter for Stable Diffusion XL to generate Simpsons-style images. Perfect for creating custom image generation models with specific artistic styles.",
        "notes": "# Diffusion LoRA Training with Stable Diffusion XL\n\nTo run this recipe, go to **Train** in the sidebar and click on Queue\n\n## Overview\nThis recipe trains a LoRA adapter for Stable Diffusion XL to generate images in the Simpsons art style.\n\n## Expected Outcome\nAfter training, the model should be able to:\n- Generate Simpsons-style images from text prompts\n- Create characters and scenes in the distinctive animation style\n- Respond to the trigger word \"<simpsons sks>\" for style activation",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "stabilityai/stable-diffusion-xl-base-1.0"
            },
            {
                "type": "dataset",
                "name": "Norod78/simpsons-blip-captions"
            },
            {
                "type": "plugin",
                "name": "diffusion_trainer"
            }
        ],
        "tasks": [
            {
                "name": "DiffusionLoRATrain",
                "task_type": "TRAIN",
                "plugin": "diffusion_trainer",
                "config_json": "{\"template_name\":\"DiffusionLoRATrain\",\"plugin_name\":\"diffusion_trainer\",\"model_name\":\"stabilityai/stable-diffusion-xl-base-1.0\",\"model_architecture\":\"StableDiffusionXLPipeline\",\"foundation_model_file_path\":\"\",\"embedding_model\":\"BAAI/bge-base-en-v1.5\",\"embedding_model_architecture\":\"BertModel\",\"embedding_model_file_path\":\"\",\"dataset_name\":\"Norod78/simpsons-blip-captions\",\"adaptor_name\":\"simpsons-adaptor\",\"trigger_word\":\"<simpsons sks>\",\"num_train_epochs\":30,\"train_batch_size\":2,\"gradient_accumulation_steps\":1,\"caption_column\":\"text\",\"image_column\":\"image\",\"caption_dropout_rate\":0.2,\"resolution\":512,\"image_interpolation_mode\":\"lanczos\",\"color_jitter_brightness\":0.1,\"color_jitter_contrast\":0.1,\"color_jitter_saturation\":0.1,\"color_jitter_hue\":0.05,\"rotation_degrees\":5,\"rotation_prob\":0.3,\"lora_r\":16,\"lora_alpha\":16,\"learning_rate\":0.00006,\"lr_scheduler\":\"constant\",\"lr_warmup_steps\":50,\"adam_beta1\":0.9,\"adam_beta2\":0.999,\"adam_weight_decay\":0.01,\"adam_epsilon\":1e-8,\"max_grad_norm\":1,\"loss_type\":\"l2\",\"huber_c\":0.1,\"prediction_type\":\"epsilon\",\"snr_gamma\":0,\"min_snr_gamma\":0,\"noise_offset\":0,\"mixed_precision\":\"no\",\"ema_decay\":0.9999,\"eval_prompt\":\"\",\"eval_steps\":1,\"eval_num_inference_steps\":50,\"eval_guidance_scale\":7.5,\"log_to_wandb\":true,\"center_crop\":false,\"random_flip\":false,\"color_jitter\":false,\"random_rotation\":false,\"enable_xformers_memory_efficient_attention\":false,\"gradient_checkpointing\":false,\"use_ema\":false,\"type\":\"LoRA\",\"chatml_formatted_column\":\"\",\"run_sweeps\":false}",
                "inputs_json": "{\"model_name\":\"stabilityai/stable-diffusion-xl-base-1.0\",\"model_architecture\":\"StableDiffusionXLPipeline\",\"dataset_name\":\"Norod78/simpsons-blip-captions\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://recipes.transformerlab.net/gemini_diffusion_image.jpeg"
    },
    {
        "id": "eval-common-benchmarks-mlx",
        "title": "Evaluate a Model on Common Benchmarks on MLX",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "description": "Performs evaluation on common benchmarks using the Eleuther AI LM Eval Harness. It evaluates the model `mlx-community/Llama-3.2-1B-Instruct-4bit` on tasks such as Winogrande, HellaSwag, and PIQA.",
        "notes": "# Model Evaluation on Common Benchmarks (MLX)\n\nTo run this experiment, go to Evaluate in the sidebar, and click on Queue. Once it is done, click on \"Detailed Report\" or \"Chart\" to see results.\n\n## Overview\nThis recipe evaluates a Llama 3.2 model on common benchmarks using the Eleuther AI LM Eval Harness.\n\n## Expected Outcome\nAfter evaluation, you can view:\n- Detailed performance reports\n- Comparative charts and metrics\n- Benchmark scores for HellaSwag, PIQA, and Winogrande",
        "zOrder": 2,
        "dependencies": [
            {
                "type": "model",
                "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
            },
            {
                "type": "plugin",
                "name": "common-eleuther-ai-lm-eval-harness-mlx"
            }
        ],
        "tasks": [
            {
                "name": "EvalOnCommonBenchmarks",
                "task_type": "EVAL",
                "plugin": "common-eleuther-ai-lm-eval-harness-mlx",
                "config_json": "{\"template_name\":\"EvalOnCommonBenchmarks\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"hellaswag,piqa,winogrande\",\"limit\":\"1\",\"run_name\":\"EvalOnCommonBenchmarks\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"EvalOnCommonBenchmarks\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"hellaswag,piqa,winogrande\",\"limit\":\"1\",\"run_name\":\"EvalOnCommonBenchmarks\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            }
        ],
        "workflows": [],
        "cardImage": "https://recipes.transformerlab.net/radialchart.png"
    },
    {
        "id": "eval-common-benchmarks-non-mlx",
        "title": "Evaluate a Model on Common Benchmarks",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "description": "Performs evaluation on common benchmarks using the Eleuther AI LM Eval Harness. It evaluates the model `unsloth/Llama-3.2-1B-Instruct` on tasks such as MMLU, Winogrande, HellaSwag, and PIQA.",
        "notes": "# Model Evaluation on Common Benchmarks\n\n To run this recipe, go to **Evaluate** in the sidebar and click on Queue\n\n## Overview\nThis recipe evaluates a Llama 3.2 model on common benchmarks using the Eleuther AI LM Eval Harness.\n\n## Expected Outcome\nAfter evaluation, you can view:\n- Detailed performance reports\n- Comparative charts and metrics\n- Benchmark scores for MMLU, Winogrande, HellaSwag, and PIQA",
        "dependencies": [
            {
                "type": "model",
                "name": "unsloth/Llama-3.2-1B-Instruct"
            },
            {
                "type": "plugin",
                "name": "common-eleuther-ai-lm-eval-harness"
            }
        ],
        "tasks": [
            {
                "name": "KindMoose",
                "task_type": "EVAL",
                "plugin": "common-eleuther-ai-lm-eval-harness",
                "config_json": "{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            }
        ],
        "workflows": [],
        "cardImage": "https://recipes.transformerlab.net/radialchart.png"
    },
    {
        "id": "fine_tune_small_mlx",
        "title": "Fine Tune a Small Language Model using MLX",
        "description": "Train a Llama 3.2 1B model to understand and answer questions about Touch Rugby rules using the MLX framework. Perfect for rule-based question answering.",
        "notes": "# What This Recipe Does\n\nThis recipe compares a base model's performance before and after fine-tuning on touch rugby data using MLX (optimized for Apple Silicon):\n\n1. **Generate Base Model Outputs** - Creates responses using the original model\n2. **Evaluate Base Model** - Measures quality using ROUGE scores\n3. **Fine-tune with LoRA** - Trains the model on touch rugby rules using MLX framework\n4. **Generate Fine-tuned Outputs** - Creates responses using the trained adaptor\n5. **Evaluate Fine-tuned Model** - Measures improved quality\n\n## How to Use\n1. Go to the **Generate** tab to create base model outputs\n2. Go to the **Evaluate** tab to evaluate the base model\n3. Go to the **Train** tab to fine-tune with LoRA\n4. Go to the **Generate** tab to create fine-tuned outputs\n5. Go to the **Evaluate** tab to see the comparison results\n\n## What You'll Learn\n- How much fine-tuning improves performance on your specific domain\n- The difference between base model and fine-tuned model outputs\n- Benefits of using MLX for efficient training on Apple Silicon",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "zOrder": 1,
        "dependencies": [
            {
                "type": "model",
                "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
            },
            {
                "type": "dataset",
                "name": "Trelis/touch-rugby-rules"
            },
            {
                "type": "plugin",
                "name": "mlx_lora_trainer"
            },
            {
                "type": "plugin",
                "name": "mlx_server"
            },
            {
                "type": "plugin",
                "name": "deepeval_objective"
            },
            {
                "type": "dataset",
                "name": "transformerlab/touch-rugby-evals"
            },
            {
                "type": "plugin",
                "name": "batched_generation_datasets"
            }
        ],
        "tasks": [
            {
                "name": "TouchRugby",
                "task_type": "TRAIN",
                "plugin": "mlx_lora_trainer",
                "config_json": "{\"template_name\":\"TouchRugby\",\"plugin_name\":\"mlx_lora_trainer\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"LlamaForCausalLM\",\"foundation_model_file_path\":\"\",\"embedding_model\":\"BAAI/bge-base-en-v1.5\",\"embedding_model_architecture\":\"BertModel\",\"embedding_model_file_path\":\"\",\"formatting_template\":\"{{prompt}}\\n{{completion}}\",\"dataset_name\":\"Trelis/touch-rugby-rules\",\"lora_layers\":\"16\",\"batch_size\":\"8\",\"learning_rate\":\"0.0001\",\"lora_rank\":\"32\",\"lora_alpha\":\"128\",\"iters\":\"120\",\"num_train_epochs\":\"-1\",\"steps_per_report\":\"10\",\"steps_per_eval\":\"20\",\"save_every\":\"10\",\"adaptor_name\":\"touch-rugby\",\"fuse_model\":\"true\",\"log_to_wandb\":\"true\",\"_tlab_recipe_datasets\":\"[object Object]\",\"_tlab_recipe_models\":\"[object Object]\",\"sweep_config\":\"{}\",\"run_sweeps\":false,\"type\":\"LoRA\"}",
                "inputs_json": "{\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"LlamaForCausalLM\",\"dataset_name\":\"Trelis/touch-rugby-rules\"}"
            },
            {
                "name": "FineTunedOutputs",
                "task_type": "EVAL",
                "plugin": "deepeval_objective",
                "config_json": "{\"template_name\":\"FineTunedOutputs\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"finetunedoutputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"FineTunedOutputs\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"FineTunedOutputs\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"finetunedoutputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"FineTunedOutputs\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "BaseModelEvals",
                "task_type": "EVAL",
                "plugin": "deepeval_objective",
                "config_json": "{\"template_name\":\"BaseModelEvals\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"basemodeloutputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelEvals\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"BaseModelEvals\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"basemodeloutputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelEvals\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "BaseModelOutputs",
                "task_type": "GENERATE",
                "plugin": "batched_generation_datasets",
                "config_json": "{\"template_name\":\"BaseModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"mlx_server\\\"}\",\"system_prompt\":\"Answer in the context of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"1\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"BaseModelOutputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelOutputs\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"BaseModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"mlx_server\\\"}\",\"system_prompt\":\"Answer in the context of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"1\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"BaseModelOutputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelOutputs\",\"generation_type\":\"scratch\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "FinedTunedModelOutputs",
                "task_type": "GENERATE",
                "plugin": "batched_generation_datasets",
                "config_json": "{\"template_name\":\"FinedTunedModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"touch-rugby\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"mlx_server\\\"}\",\"system_prompt\":\"Answer in the context of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"1\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"FineTunedOutputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"FinedTunedModelOutputs\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"FinedTunedModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"touch-rugby\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"mlx_server\\\"}\",\"system_prompt\":\"Answer in the context of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"1\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"FineTunedOutputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"FinedTunedModelOutputs\",\"generation_type\":\"scratch\"}}",
                "inputs_json": "{}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1558151507-c1aa3d917dbb?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "fine_tune_existing_model",
        "title": "Fine-tune an Existing Model",
        "description": "Adapt a pre-trained model to your specific needs using LoRA. Save time and resources by leveraging existing knowledge.",
        "notes": "# What This Recipe Does\n\nThis recipe compares a base model's performance before and after fine-tuning on touch rugby data:\n\n1. **Generate Base Model Outputs** - Creates responses using the original model\n2. **Evaluate Base Model** - Measures quality using ROUGE scores\n3. **Fine-tune with LoRA** - Trains the model on touch rugby rules (efficient training method)\n4. **Generate Fine-tuned Outputs** - Creates responses using the trained adaptor\n5. **Evaluate Fine-tuned Model** - Measures improved quality\n\n## How to Use\n1. Go to the **Generate** tab to create base model outputs\n2. Go to the **Evaluate** tab to evaluate the base model\n3. Go to the **Train** tab to fine-tune with LoRA\n4. Go to the **Generate** tab to create fine-tuned outputs\n5. Go to the **Evaluate** tab to see the comparison results\n\n## What You'll Learn\n- How much fine-tuning improves performance on your specific domain\n- The difference between base model and fine-tuned model outputs",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "unsloth/Llama-3.2-1B-Instruct"
            },
            {
                "type": "dataset",
                "name": "Trelis/touch-rugby-rules"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            },
            {
                "type": "plugin",
                "name": "fastchat_server"
            },
            {
                "type": "plugin",
                "name": "deepeval_objective"
            },
            {
                "type": "dataset",
                "name": "transformerlab/touch-rugby-evals"
            },
            {
                "type": "plugin",
                "name": "batched_generation_datasets"
            }
        ],
        "tasks": [
            {
                "name": "BrilliantBaboon",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\":\"BrilliantBaboon\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"foundation_model_file_path\":\"\",\"embedding_model\":\"BAAI/bge-base-en-v1.5\",\"embedding_model_architecture\":\"BertModel\",\"embedding_model_file_path\":\"\",\"formatting_template\":\"{{prompt}}\\n{{completion}}\",\"dataset_name\":\"Trelis/touch-rugby-rules\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate_schedule\":\"constant\",\"learning_rate\":\"0.00005\",\"num_train_epochs\":\"200\",\"max_steps\":\"-1\",\"lora_r\":\"4\",\"lora_alpha\":\"8\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"touch-rugby-rules\",\"log_to_wandb\":\"true\",\"fuse_model\":\"true\",\"sweep_config\":\"{}\",\"run_sweeps\":false,\"type\":\"LoRA\"}",
                "inputs_json": "{\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"dataset_name\":\"Trelis/touch-rugby-rules\"}"
            },
            {
                "name": "EvalFineTunedRugby",
                "task_type": "EVAL",
                "plugin": "deepeval_objective",
                "config_json": "{\"template_name\":\"EvalFineTunedRugby\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"finetuned_rugby_outputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"EvalFineTunedRugby\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"EvalFineTunedRugby\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"finetuned_rugby_outputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"EvalFineTunedRugby\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "EvalBaseModel",
                "task_type": "EVAL",
                "plugin": "deepeval_objective",
                "config_json": "{\"template_name\":\"EvalBaseModel\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"base_rugby_outputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"EvalBaseModel\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"EvalBaseModel\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"base_rugby_outputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"EvalBaseModel\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "TrainedAdaptorOutputs",
                "task_type": "GENERATE",
                "plugin": "batched_generation_datasets",
                "config_json": "{\"template_name\":\"TrainedAdaptorOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"fastchat_server\\\"}\",\"system_prompt\":\"Answer in terms of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"128\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"finetuned_rugby_outputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"TrainedAdaptorOutputs\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"TrainedAdaptorOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"fastchat_server\\\"}\",\"system_prompt\":\"Answer in terms of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"128\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"finetuned_rugby_outputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"TrainedAdaptorOutputs\",\"generation_type\":\"scratch\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "BaseModelOutputs",
                "task_type": "GENERATE",
                "plugin": "batched_generation_datasets",
                "config_json": "{\"template_name\":\"BaseModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"fastchat_server\\\"}\",\"system_prompt\":\"Answer in terms of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"128\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"base_rugby_outputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelOutputs\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"BaseModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"fastchat_server\\\"}\",\"system_prompt\":\"Answer in terms of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"128\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"base_rugby_outputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelOutputs\",\"generation_type\":\"scratch\"}}",
                "inputs_json": "{}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1561375996-8bbec3f2a481?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "ml_qa",
        "title": "Machine Learning Q&A",
        "description": "Train a Qwen 2.5 model to provide expert-level answers to machine learning questions, suitable for both beginners and advanced practitioners.",
        "notes": "# Machine Learning Q&A with Qwen 2.5\n\n<-- To run this recipe, go to **Train** in the sidebar and click on Queue\n\n## Overview\nThis recipe fine-tunes a Qwen 2.5 model to become a specialized machine learning assistant.\n\n## Expected Outcome\nAfter training, the model should be able to:\n- Provide detailed ML explanations\n- Answer implementation questions\n- Guide through ML concepts progressively",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "dataset",
                "name": "win-wang/Machine_Learning_QA_Collection"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "MachineLearningQnA",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"MachineLearningQnA\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"model_architecture\": \"Qwen2ForCausalLM\", \"formatting_template\": \"{{text}}\\n\", \"dataset_name\": \"win-wang/Machine_Learning_QA_Collection\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"1\", \"learning_rate\": \"0.00005\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"16\", \"lora_alpha\": \"64\", \"lora_dropout\": \"0.1\", \"adaptor_name\": \"ML-QA\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"win-wang/Machine_Learning_QA_Collection\\\", \\\"path\\\": \\\"win-wang/Machine_Learning_QA_Collection\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\", \\\"path\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\"}\"}",
                "inputs_json": "{\"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"model_architecture\": \"Qwen2ForCausalLM\", \"dataset_name\": \"win-wang/Machine_Learning_QA_Collection\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1557562645-4eee56b29bc1?q=80&w=1935&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "python_code_completion",
        "title": "Python Code Completion",
        "description": "Train a SmolLM Base model to provide intelligent Python code completions, suggestions, and assistance. Ideal for developers looking for an efficient coding assistant.",
        "notes": "# Python Code Completion with SmolLM\n\n<-- To run this recipe, go to **Train** in the sidebar and click on Queue\n\n## Overview\nThis recipe fine-tunes a SmolLM Base model to become a specialized Python code completion assistant.\n\n## Expected Outcome\nAfter training, the model should be able to:\n- Provide context-aware code completions\n- Suggest appropriate Python syntax\n- Complete common programming patterns",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "HuggingFaceTB/SmolLM2-135M"
            },
            {
                "type": "dataset",
                "name": "flytech/python-codes-25k"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "train_smollm_python_completion",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"PythonCompletion\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"HuggingFaceTB/SmolLM2-135M\", \"model_architecture\": \"LlamaForCausalLM\", \"formatting_template\": \"{{output}}\\n\", \"dataset_name\": \"flytech/python-codes-25k\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"4\", \"learning_rate\": \"0.0005\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"64\", \"lora_alpha\": \"128\", \"lora_dropout\": \"0.05\", \"adaptor_name\": \"python\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"flytech/python-codes-25k\\\", \\\"path\\\": \\\"flytech/python-codes-25k\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"HuggingFaceTB/SmolLM2-135M\\\", \\\"path\\\": \\\"HuggingFaceTB/SmolLM2-135M\\\"}\"}",
                "inputs_json": "{\"model_name\": \"HuggingFaceTB/SmolLM2-135M\", \"model_architecture\": \"LlamaForCausalLM\", \"dataset_name\": \"flytech/python-codes-25k\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?q=80&w=2069&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "quantize_model",
        "title": "Quantize a Model",
        "description": "Optimize your model for faster inference and reduced size using quantization tools.",
        "notes": "# Model Quantization\n\nThis recipe quantizes a model to reduce its size and improve inference speed while maintaining performance.\n\n## How to Use\nTo get a quantized model, simply go to the **Export** tab and run the export task. Once it is complete, you can go to the **Model Zoo** -> **Generated** to see your newly generated model.\n\n## What it does\n- Converts your model to GGUF format with q8_0 quantization\n- Reduces model size for faster loading\n- Maintains good performance with minimal accuracy loss",
        "requiredMachineArchitecture": [
            "mlx",
            "cuda"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "unsloth/Llama-3.2-1B-Instruct"
            },
            {
                "type": "plugin",
                "name": "gguf_exporter"
            }
        ],
        "tasks": [
            {
                "name": "ExportGGUFTask",
                "task_type": "EXPORT",
                "plugin": "gguf_exporter",
                "config_json": "{\"plugin_name\":\"gguf_exporter\",\"input_model_id\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_path\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_architecture\":\"LlamaForCausalLM\",\"output_model_id\":\"Llama-3.2-1B-Instruct-1752517918-q8_0.gguf\",\"output_model_architecture\":\"GGUF\",\"output_model_name\":\"Llama-3.2-1B-Instruct - GGUF - q8_0\",\"output_model_path\":\"/models/Llama-3.2-1B-Instruct-1752517918-q8_0.gguf\",\"output_filename\":\"Llama-3.2-1B-Instruct-1752517918-q8_0.gguf\",\"script_directory\":\"/plugins/gguf_exporter\",\"params\":{\"outtype\":\"q8_0\"},\"run_name\":\"ExportGGUFTask\"}",
                "inputs_json": "{\"input_model_id\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_path\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_architecture\":\"LlamaForCausalLM\",\"plugin_name\":\"gguf_exporter\",\"plugin_architecture\":\"GGUF\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://recipes.transformerlab.net/quantization.png"
    },
    {
        "id": "conversational_intelligence",
        "title": "Train a Model to be Conversationally Intelligent",
        "description": "Enhance a SmolLM model with advanced conversational abilities and structured response formatting using XML tags, ideal for creating a sophisticated dialogue agent.",
        "notes": "# Conversational Intelligence with SmolLM\n\n<-- To run this recipe, go to **Train** in the sidebar and click on Queue\n\n## Overview\nThis recipe develops a SmolLM model into a sophisticated conversational agent using structured dialogue formats.\n\n## Expected Outcome\nAfter training, the model should be able to:\n- Maintain structured conversations\n- Generate well-formatted responses\n- Handle diverse dialogue scenarios",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "HuggingFaceTB/SmolLM2-135M"
            },
            {
                "type": "dataset",
                "name": "nickrosh/Evol-Instruct-Code-80k-v1"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "InstructTuning",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"InstructTuning\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"HuggingFaceTB/SmolLM2-135M\", \"model_architecture\": \"LlamaForCausalLM\", \"formatting_template\": \"<User>\\n{{instruction}}\\n</User>\\n<Assistant>\\n{{output}}\\n</Assistant>\\n\", \"dataset_name\": \"nickrosh/Evol-Instruct-Code-80k-v1\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"4\", \"learning_rate\": \"0.00003\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"4\", \"lora_alpha\": \"16\", \"lora_dropout\": \"0.05\", \"adaptor_name\": \"instruct\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"nickrosh/Evol-Instruct-Code-80k-v1\\\", \\\"path\\\": \\\"nickrosh/Evol-Instruct-Code-80k-v1\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"HuggingFaceTB/SmolLM2-135M\\\", \\\"path\\\": \\\"HuggingFaceTB/SmolLM2-135M\\\"}\"}",
                "inputs_json": "{\"model_name\": \"HuggingFaceTB/SmolLM2-135M\", \"model_architecture\": \"LlamaForCausalLM\", \"dataset_name\": \"nickrosh/Evol-Instruct-Code-80k-v1\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1573497620053-ea5300f94f21?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "pirate_speech",
        "title": "Train a Model to Speak like a Pirate",
        "description": "Transform a SmolLM model into a charismatic pirate conversationalist, perfect for creating engaging and entertaining interactions with a nautical twist.",
        "notes": "# Pirate Speech with SmolLM\n\n<-- To run this recipe, go to **Train** in the sidebar and click on Queue\n\n## Overview\nThis recipe transforms a SmolLM model into an engaging pirate-speaking assistant using specialized dialogue data.\n\n## Expected Outcome\nAfter training, the model should be able to:\n- Respond in consistent pirate dialect\n- Maintain helpful information delivery\n- Create engaging pirate-themed interactions",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "HuggingFaceTB/SmolLM-135M-Instruct"
            },
            {
                "type": "dataset",
                "name": "Peyton3995/dolly-15k-mistral-pirate"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "PirateSpeech",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"PirateSpeech\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"HuggingFaceTB/SmolLM-135M-Instruct\", \"model_architecture\": \"LlamaForCausalLM\", \"formatting_template\": \"<instruction>\\n{{instruction}}\\n</instruction>\\n<response>\\n{{response}}\\n</response>\", \"dataset_name\": \"Peyton3995/dolly-15k-mistral-pirate\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"4\", \"learning_rate_schedule\": \"cosine\", \"learning_rate\": \"0.01\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"64\", \"lora_alpha\": \"128\", \"lora_dropout\": \"0.05\", \"adaptor_name\": \"Pirate_Speech\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"Peyton3995/dolly-15k-mistral-pirate\\\", \\\"path\\\": \\\"Peyton3995/dolly-15k-mistral-pirate\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"HuggingFaceTB/SmolLM-135M-Instruct\\\", \\\"path\\\": \\\"HuggingFaceTB/SmolLM-135M-Instruct\\\"}\"}",
                "inputs_json": "{\"model_name\": \"HuggingFaceTB/SmolLM-135M-Instruct\", \"model_architecture\": \"LlamaForCausalLM\", \"dataset_name\": \"Peyton3995/dolly-15k-mistral-pirate\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1652447275071-4bf852aebdc5?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    }
]