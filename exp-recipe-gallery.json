[
    {
        "id": "answer_sql_queries",
        "title": "Answer SQL Queries",
        "description": "Train a Qwen 2.5 model to excel at SQL query generation, understanding, and optimization across various database scenarios.",
        "notes": "# SQL Query Assistant with Qwen 2.5\n\n<-- To run this recipe, go to **Train** in the sidebar and click on Queue\n\n## Overview\nThis recipe fine-tunes a Qwen 2.5 model to become a specialized SQL query assistant.\n\n## Expected Outcome\nAfter training, the model should be able to:\n- Write efficient SQL queries\n- Explain query optimization\n- Handle complex database operations",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "dataset",
                "name": "mlx-community/wikisql"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "WikiSQL",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"Wiki SQL\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"model_architecture\": \"Qwen2ForCausalLM\", \"formatting_template\": \"{{text}} ;\", \"dataset_name\": \"mlx-community/wikisql\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"1\", \"learning_rate\": \"0.005\", \"num_train_epochs\": \"2\", \"max_steps\": \"-1\", \"lora_r\": \"32\", \"lora_alpha\": \"64\", \"lora_dropout\": \"0.1\", \"adaptor_name\": \"WikiSQL\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"mlx-community/wikisql\\\", \\\"path\\\": \\\"mlx-community/wikisql\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\", \\\"path\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\"}\"}",
                "inputs_json": "{\"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"model_architecture\": \"Qwen2ForCausalLM\", \"dataset_name\": \"mlx-community/wikisql\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1683322499436-f4383dd59f5a?q=80&w=2071&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "dialogue_summarizing",
        "title": "Dialogue Summarizing",
        "description": "Fine-tune a TinyLlama model to create concise, accurate summaries of conversations and dialogues. Perfect for chat logs, meeting transcripts, and customer service interactions.",
        "notes": "# Dialogue Summarization with TinyLlama\n\n## Overview\nThis recipe demonstrates how to fine-tune a TinyLlama model specifically for dialogue summarization using the popular SAMSum dataset.\n\n<-- To run this recipe, go the **Training** tab and press Queue.\n\n## Background\n- TinyLlama is optimized for efficiency while maintaining good performance\n- Uses LoRA for memory-efficient fine-tuning\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Generate concise summaries of conversations\n- Maintain key points and context\n- Handle various dialogue formats and styles",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
            },
            {
                "type": "dataset",
                "name": "knkarthick/samsum"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "train_tinyllama_summarizer",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"DialogueSummarizing\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"model_architecture\": \"LlamaForCausalLM\", \"formatting_template\": \"Instruction: Summarize the Following\\nPrompt: {{dialogue}}\\nGeneration: {{summary}}\", \"dataset_name\": \"knkarthick/samsum\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"4\", \"learning_rate\": \"0.00005\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"32\", \"lora_alpha\": \"64\", \"lora_dropout\": \"0.05\", \"adaptor_name\": \"Summarizer\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"knkarthick/samsum\\\", \\\"path\\\": \\\"knkarthick/samsum\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\\\", \\\"path\\\": \\\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\\\"}\"}",
                "inputs_json": "{\"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"model_architecture\": \"LlamaForCausalLM\", \"dataset_name\": \"knkarthick/samsum\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1590650046871-92c887180603?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "eval-common-benchmarks-non-mlx",
        "title": "Evaluate a Model on Common Benchmarks on MLX",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "description": "Performs evaluation on common benchmarks using the Eleuther AI LM Eval Harness. It evaluates the model `mlx-community/Llama-3.2-1B-Instruct-4bit` on tasks such as Winogrande, HellaSwag, and PIQA.",
        "notes": "In this simple eval experiment, we set up some common evaluations against\nthe open source mlx-community/Llama-3.2-1B-Instruct-4bit model.\n\nTo run it, go to Evaluate in the sidebar, and click on Queue. Once it is done, click on\n\"Detailed Report\" or \"Chart\" to see results.",
        "zOrder": 2,
        "dependencies": [
            {
                "type": "model",
                "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
            },
            {
                "type": "plugin",
                "name": "common-eleuther-ai-lm-eval-harness-mlx"
            }
        ],
        "tasks": [
            {
                "name": "EvalOnCommonBenchmarks",
                "task_type": "EVAL",
                "plugin": "common-eleuther-ai-lm-eval-harness-mlx",
                "config_json": "{\"template_name\":\"EvalOnCommonBenchmarks\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"hellaswag,piqa,winogrande\",\"limit\":\"1\",\"run_name\":\"EvalOnCommonBenchmarks\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"EvalOnCommonBenchmarks\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"hellaswag,piqa,winogrande\",\"limit\":\"1\",\"run_name\":\"EvalOnCommonBenchmarks\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            }
        ],
        "workflows": [
            {
                "name": "eval-on-common-benchmarks",
                "config": {
                    "nodes": [
                        {
                            "type": "START",
                            "id": "99b97abd-82de-4745-b64a-6540801261c1",
                            "name": "START",
                            "out": [
                                "06334a95-01c4-4ece-82fc-9a107a4036e2"
                            ]
                        },
                        {
                            "name": "Eval on Harness Benchmarks",
                            "task": "EvalOnCommonBenchmarks",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": -75,
                                    "y": 105
                                }
                            },
                            "id": "06334a95-01c4-4ece-82fc-9a107a4036e2",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://recipes.transformerlab.net/radialchart.png"
    },
    {
        "id": "eval-common-benchmarks-non-mlx",
        "title": "Evaluate a Model on Common Benchmarks",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "description": "Performs evaluation on common benchmarks using the Eleuther AI LM Eval Harness. It evaluates the model `unsloth/Llama-3.2-1B-Instruct` on tasks such as MMLU, Winogrande, HellaSwag, and PIQA.",
        "notes": "",
        "dependencies": [
            {
                "type": "model",
                "name": "unsloth/Llama-3.2-1B-Instruct"
            },
            {
                "type": "plugin",
                "name": "common-eleuther-ai-lm-eval-harness"
            }
        ],
        "tasks": [
            {
                "name": "KindMoose",
                "task_type": "EVAL",
                "plugin": "common-eleuther-ai-lm-eval-harness",
                "config_json": "{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            }
        ],
        "workflows": [
            {
                "name": "evaluate-on-common-benchmarks",
                "config": {
                    "nodes": [
                        {
                            "type": "START",
                            "id": "bc9dc3a4-afba-4956-a55f-bd51e96da24f",
                            "name": "START",
                            "out": [
                                "009309fa-1ed5-42bc-be5d-b84e32772bf1"
                            ]
                        },
                        {
                            "name": "EVAL HARNESS",
                            "task": "KindMoose",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": -60,
                                    "y": 75
                                }
                            },
                            "id": "009309fa-1ed5-42bc-be5d-b84e32772bf1",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1589595427524-2ddaf2d43fc9?q=80&w=1744&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "fine_tune_small_mlx",
        "title": "Fine Tune a Small Language Model using MLX",
        "description": "Train a Llama 3.2 1B model to understand and answer questions about Touch Rugby rules using the MLX framework. Perfect for rule-based question answering.",
        "notes": "# What This Recipe Does\n\nThis recipe compares a base model's performance before and after fine-tuning on touch rugby data using MLX (optimized for Apple Silicon):\n\n1. **Generate Base Model Outputs** - Creates responses using the original model\n2. **Evaluate Base Model** - Measures quality using ROUGE scores\n3. **Fine-tune with LoRA** - Trains the model on touch rugby rules using MLX framework\n4. **Generate Fine-tuned Outputs** - Creates responses using the trained adaptor\n5. **Evaluate Fine-tuned Model** - Measures improved quality\n\n## How to Use\n1. Go to the **Workflows** tab and run the \"fine_tune_model\" workflow\n2. Wait for all steps to complete\n3. Switch to the **Evaluate** tab to see the comparison results\n\n## What You'll Learn\n- How much fine-tuning improves performance on your specific domain\n- The difference between base model and fine-tuned model outputs\n- Benefits of using MLX for efficient training on Apple Silicon",
        "requiredMachineArchitecture": [
            "mlx"
        ],
        "zOrder": 1,
        "dependencies": [
            {
                "type": "model",
                "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
            },
            {
                "type": "dataset",
                "name": "Trelis/touch-rugby-rules"
            },
            {
                "type": "plugin",
                "name": "mlx_lora_trainer"
            },
            {
                "type": "plugin",
                "name": "mlx_server"
            },
            {
                "type": "plugin",
                "name": "deepeval_objective"
            },
            {
                "type": "dataset",
                "name": "transformerlab/touch-rugby-evals"
            },
            {
                "type": "plugin",
                "name": "batched_generation_datasets"
            }
        ],
        "tasks": [
            {
                "name": "TouchRugby",
                "task_type": "TRAIN",
                "plugin": "mlx_lora_trainer",
                "config_json": "{\"template_name\":\"TouchRugby\",\"plugin_name\":\"mlx_lora_trainer\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"LlamaForCausalLM\",\"foundation_model_file_path\":\"\",\"embedding_model\":\"BAAI/bge-base-en-v1.5\",\"embedding_model_architecture\":\"BertModel\",\"embedding_model_file_path\":\"\",\"formatting_template\":\"{{prompt}}\\n{{completion}}\",\"dataset_name\":\"Trelis/touch-rugby-rules\",\"lora_layers\":\"16\",\"batch_size\":\"8\",\"learning_rate\":\"0.0001\",\"lora_rank\":\"32\",\"lora_alpha\":\"128\",\"iters\":\"120\",\"num_train_epochs\":\"-1\",\"steps_per_report\":\"10\",\"steps_per_eval\":\"20\",\"save_every\":\"10\",\"adaptor_name\":\"touch-rugby\",\"fuse_model\":\"true\",\"log_to_wandb\":\"true\",\"_tlab_recipe_datasets\":\"[object Object]\",\"_tlab_recipe_models\":\"[object Object]\",\"sweep_config\":\"{}\",\"run_sweeps\":false,\"type\":\"LoRA\"}",
                "inputs_json": "{\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"LlamaForCausalLM\",\"dataset_name\":\"Trelis/touch-rugby-rules\"}"
            },
            {
                "name": "FineTunedOutputs",
                "task_type": "EVAL",
                "plugin": "deepeval_objective",
                "config_json": "{\"template_name\":\"FineTunedOutputs\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"finetunedoutputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"FineTunedOutputs\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"FineTunedOutputs\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"finetunedoutputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"FineTunedOutputs\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "BaseModelEvals",
                "task_type": "EVAL",
                "plugin": "deepeval_objective",
                "config_json": "{\"template_name\":\"BaseModelEvals\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"basemodeloutputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelEvals\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"BaseModelEvals\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"basemodeloutputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelEvals\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "BaseModelOutputs",
                "task_type": "GENERATE",
                "plugin": "batched_generation_datasets",
                "config_json": "{\"template_name\":\"BaseModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"mlx_server\\\"}\",\"system_prompt\":\"Answer in the context of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"1\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"BaseModelOutputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelOutputs\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"BaseModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"mlx_server\\\"}\",\"system_prompt\":\"Answer in the context of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"1\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"BaseModelOutputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelOutputs\",\"generation_type\":\"scratch\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "FinedTunedModelOutputs",
                "task_type": "GENERATE",
                "plugin": "batched_generation_datasets",
                "config_json": "{\"template_name\":\"FinedTunedModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"touch-rugby\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"mlx_server\\\"}\",\"system_prompt\":\"Answer in the context of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"1\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"FineTunedOutputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"FinedTunedModelOutputs\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"FinedTunedModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"model_adapter\":\"touch-rugby\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"mlx_server\\\"}\",\"system_prompt\":\"Answer in the context of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"1\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"FineTunedOutputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"FinedTunedModelOutputs\",\"generation_type\":\"scratch\"}}",
                "inputs_json": "{}"
            }
        ],
        "workflows": [
            {
                "name": "fine_tune_model",
                "config": {
                    "nodes": [
                        {
                            "type": "START",
                            "id": "c2a75b7e-ed71-40d6-ad35-e4dffc9f22f0",
                            "name": "START",
                            "out": [
                                "0f143d9a-0abe-4fbf-8254-d423b283ee64"
                            ],
                            "metadata": {
                                "position": {
                                    "x": -15,
                                    "y": -180
                                }
                            }
                        },
                        {
                            "name": "GenerateBaseOutputs",
                            "task": "BaseModelOutputs",
                            "type": "GENERATE",
                            "metadata": {
                                "position": {
                                    "x": -75,
                                    "y": -105
                                }
                            },
                            "id": "0f143d9a-0abe-4fbf-8254-d423b283ee64",
                            "out": [
                                "9d107acd-3815-4b45-b87b-d52e249a3799"
                            ]
                        },
                        {
                            "name": "EvalBaseOutputs",
                            "task": "BaseModelEvals",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": -75,
                                    "y": 30
                                }
                            },
                            "id": "9d107acd-3815-4b45-b87b-d52e249a3799",
                            "out": [
                                "5c04dd3b-4e3e-47a2-bfdb-e97bf003b357"
                            ]
                        },
                        {
                            "name": "TrainLoRA",
                            "task": "TouchRugby",
                            "type": "TRAIN",
                            "metadata": {
                                "position": {
                                    "x": -75,
                                    "y": 165
                                }
                            },
                            "id": "5c04dd3b-4e3e-47a2-bfdb-e97bf003b357",
                            "out": [
                                "3d60693d-88be-4f38-bf51-415d6a68d709"
                            ]
                        },
                        {
                            "name": "GenerateFineTunedOutputs",
                            "task": "FinedTunedModelOutputs",
                            "type": "GENERATE",
                            "metadata": {
                                "position": {
                                    "x": -90,
                                    "y": 315
                                }
                            },
                            "id": "3d60693d-88be-4f38-bf51-415d6a68d709",
                            "out": [
                                "4ef42b37-fb9a-480e-b056-47e25584f7cd"
                            ]
                        },
                        {
                            "name": "EvalFineTuned",
                            "task": "FineTunedOutputs",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": -75,
                                    "y": 465
                                }
                            },
                            "id": "4ef42b37-fb9a-480e-b056-47e25584f7cd",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1558151507-c1aa3d917dbb?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "fine_tune_existing_model",
        "title": "Fine-tune an Existing Model",
        "description": "Adapt a pre-trained model to your specific needs using LoRA. Save time and resources by leveraging existing knowledge.",
        "notes": "# What This Recipe Does\n\nThis recipe compares a base model's performance before and after fine-tuning on touch rugby data:\n\n1. **Generate Base Model Outputs** - Creates responses using the original model\n2. **Evaluate Base Model** - Measures quality using ROUGE scores\n3. **Fine-tune with LoRA** - Trains the model on touch rugby rules (efficient training method)\n4. **Generate Fine-tuned Outputs** - Creates responses using the trained adaptor\n5. **Evaluate Fine-tuned Model** - Measures improved quality\n\n## How to Use\n1. Go to the **Workflows** tab and run the \"fine_tune_model\" workflow\n2. Wait for all steps to complete\n3. Switch to the **Evaluate** tab to see the comparison results\n\n## What You'll Learn\n- How much fine-tuning improves performance on your specific domain\n- The difference between base model and fine-tuned model outputs",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "unsloth/Llama-3.2-1B-Instruct"
            },
            {
                "type": "dataset",
                "name": "Trelis/touch-rugby-rules"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            },
            {
                "type": "plugin",
                "name": "fastchat_server"
            },
            {
                "type": "plugin",
                "name": "deepeval_objective"
            },
            {
                "type": "dataset",
                "name": "transformerlab/touch-rugby-evals"
            },
            {
                "type": "plugin",
                "name": "batched_generation_datasets"
            }
        ],
        "tasks": [
            {
                "name": "BrilliantBaboon",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\":\"BrilliantBaboon\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"foundation_model_file_path\":\"\",\"embedding_model\":\"BAAI/bge-base-en-v1.5\",\"embedding_model_architecture\":\"BertModel\",\"embedding_model_file_path\":\"\",\"formatting_template\":\"{{prompt}}\\n{{completion}}\",\"dataset_name\":\"Trelis/touch-rugby-rules\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate_schedule\":\"constant\",\"learning_rate\":\"0.00005\",\"num_train_epochs\":\"200\",\"max_steps\":\"-1\",\"lora_r\":\"4\",\"lora_alpha\":\"8\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"touch-rugby-rules\",\"log_to_wandb\":\"true\",\"fuse_model\":\"true\",\"sweep_config\":\"{}\",\"run_sweeps\":false,\"type\":\"LoRA\"}",
                "inputs_json": "{\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"dataset_name\":\"Trelis/touch-rugby-rules\"}"
            },
            {
                "name": "EvalFineTunedRugby",
                "task_type": "EVAL",
                "plugin": "deepeval_objective",
                "config_json": "{\"template_name\":\"EvalFineTunedRugby\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"finetuned_rugby_outputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"EvalFineTunedRugby\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"EvalFineTunedRugby\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"finetuned_rugby_outputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"EvalFineTunedRugby\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "EvalBaseModel",
                "task_type": "EVAL",
                "plugin": "deepeval_objective",
                "config_json": "{\"template_name\":\"EvalBaseModel\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"base_rugby_outputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"EvalBaseModel\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"EvalBaseModel\",\"plugin_name\":\"deepeval_objective\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"limit\":\"1\",\"dataset_split\":\"train\",\"tasks\":\"Rouge\",\"dataset_name\":\"base_rugby_outputs\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"EvalBaseModel\",\"predefined_tasks\":\"\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "TrainedAdaptorOutputs",
                "task_type": "GENERATE",
                "plugin": "batched_generation_datasets",
                "config_json": "{\"template_name\":\"TrainedAdaptorOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"fastchat_server\\\"}\",\"system_prompt\":\"Answer in terms of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"128\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"finetuned_rugby_outputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"TrainedAdaptorOutputs\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"TrainedAdaptorOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"touch-rugby-rules\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"fastchat_server\\\"}\",\"system_prompt\":\"Answer in terms of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"128\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"finetuned_rugby_outputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"TrainedAdaptorOutputs\",\"generation_type\":\"scratch\"}}",
                "inputs_json": "{}"
            },
            {
                "name": "BaseModelOutputs",
                "task_type": "GENERATE",
                "plugin": "batched_generation_datasets",
                "config_json": "{\"template_name\":\"BaseModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"fastchat_server\\\"}\",\"system_prompt\":\"Answer in terms of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"128\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"base_rugby_outputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelOutputs\",\"generation_type\":\"scratch\",\"script_parameters\":{\"template_name\":\"BaseModelOutputs\",\"plugin_name\":\"batched_generation_datasets\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"model_adapter\":\"\",\"generation_model\":\"{\\\"provider\\\":\\\"local\\\",\\\"model_server\\\":\\\"fastchat_server\\\"}\",\"system_prompt\":\"Answer in terms of touch rugby\",\"input_column\":\"input\",\"output_column\":\"output\",\"batch_size\":\"128\",\"temperature\":\"0.7\",\"top_p\":\"1\",\"max_tokens\":\"1024\",\"dataset_split\":\"train\",\"output_dataset_name\":\"base_rugby_outputs\",\"dataset_name\":\"transformerlab/touch-rugby-evals\",\"_dataset_display_message\":\"Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.\",\"run_name\":\"BaseModelOutputs\",\"generation_type\":\"scratch\"}}",
                "inputs_json": "{}"
            }
        ],
        "workflows": [
            {
                "name": "fine_tune_model",
                "config": {
                    "nodes": [
                        {
                            "type": "START",
                            "id": "b157b5ea-8778-486a-a94f-fef2cad1d07b",
                            "name": "START",
                            "out": [
                                "1b290a1e-aab3-494c-9502-dff4460db4b7"
                            ],
                            "metadata": {
                                "position": {
                                    "x": -30,
                                    "y": -150
                                }
                            }
                        },
                        {
                            "name": "GenerateBaseOutputs",
                            "task": "BaseModelOutputs",
                            "type": "GENERATE",
                            "metadata": {
                                "position": {
                                    "x": -90,
                                    "y": -75
                                }
                            },
                            "id": "1b290a1e-aab3-494c-9502-dff4460db4b7",
                            "out": [
                                "6e5ade62-1163-4e60-9fe5-38ed36adc6a9"
                            ]
                        },
                        {
                            "name": "EvalBaseOutputs",
                            "task": "EvalBaseModel",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": -90,
                                    "y": 45
                                }
                            },
                            "id": "6e5ade62-1163-4e60-9fe5-38ed36adc6a9",
                            "out": [
                                "969ef1ac-0e66-4f72-85a9-c4a9d08fa0fc"
                            ]
                        },
                        {
                            "name": "TrainTouchRugbyRules",
                            "task": "BrilliantBaboon",
                            "type": "TRAIN",
                            "metadata": {
                                "position": {
                                    "x": -90,
                                    "y": 195
                                }
                            },
                            "id": "969ef1ac-0e66-4f72-85a9-c4a9d08fa0fc",
                            "out": [
                                "53d4a815-6ef8-47cd-b341-24ae233c4c04"
                            ]
                        },
                        {
                            "name": "GenerateTrainedAdaptorOutputs",
                            "task": "TrainedAdaptorOutputs",
                            "type": "GENERATE",
                            "metadata": {
                                "position": {
                                    "x": -105,
                                    "y": 330
                                }
                            },
                            "id": "53d4a815-6ef8-47cd-b341-24ae233c4c04",
                            "out": [
                                "1e4e9270-4ac8-4870-a6e4-21fe04684f51"
                            ]
                        },
                        {
                            "name": "EvalTrainedAdaptor",
                            "task": "EvalFineTunedRugby",
                            "type": "EVAL",
                            "metadata": {
                                "position": {
                                    "x": -90,
                                    "y": 480
                                }
                            },
                            "id": "1e4e9270-4ac8-4870-a6e4-21fe04684f51",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://images.unsplash.com/photo-1561375996-8bbec3f2a481?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "ml_qa",
        "title": "Machine Learning Q&A",
        "description": "Train a Qwen 2.5 model to provide expert-level answers to machine learning questions, suitable for both beginners and advanced practitioners.",
        "notes": "# Machine Learning Q&A with Qwen 2.5\n\n## Overview\nThis recipe fine-tunes a Qwen 2.5 model to become a specialized machine learning assistant.\n\n## Important Considerations\n- Comprehensive ML knowledge coverage\n- Balanced between theoretical and practical knowledge\n- Suitable for various ML expertise levels\n\n## Training Tips\n- Focus on explanation clarity\n- Balance technical depth with accessibility\n- Validate answers across ML domains\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Provide detailed ML explanations\n- Answer implementation questions\n- Guide through ML concepts progressively",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "Qwen/Qwen2.5-1.5B-Instruct"
            },
            {
                "type": "dataset",
                "name": "win-wang/Machine_Learning_QA_Collection"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "MachineLearningQnA",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"MachineLearningQnA\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"model_architecture\": \"Qwen2ForCausalLM\", \"formatting_template\": \"{{text}}\\n\", \"dataset_name\": \"win-wang/Machine_Learning_QA_Collection\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"1\", \"learning_rate\": \"0.00005\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"16\", \"lora_alpha\": \"64\", \"lora_dropout\": \"0.1\", \"adaptor_name\": \"ML-QA\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"win-wang/Machine_Learning_QA_Collection\\\", \\\"path\\\": \\\"win-wang/Machine_Learning_QA_Collection\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\", \\\"path\\\": \\\"Qwen/Qwen2.5-1.5B-Instruct\\\"}\"}",
                "inputs_json": "{\"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"model_architecture\": \"Qwen2ForCausalLM\", \"dataset_name\": \"win-wang/Machine_Learning_QA_Collection\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1557562645-4eee56b29bc1?q=80&w=1935&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "python_code_completion",
        "title": "Python Code Completion",
        "description": "Train a SmolLM Base model to provide intelligent Python code completions, suggestions, and assistance. Ideal for developers looking for an efficient coding assistant.",
        "notes": "# Python Code Completion Training\n\n## Overview\nThis recipe fine-tunes a SmolLM Base model to become a specialized Python code completion assistant.\n\n## Important Considerations\n- SmolLM is designed for efficient inference\n- Dataset contains diverse Python coding examples\n- Model learns common Python patterns and best practices\n\n## Training Tips\n- Focus on code context understanding\n- Balance between common and specialized completions\n- Monitor completion accuracy and relevance\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Provide context-aware code completions\n- Suggest appropriate Python syntax\n- Complete common programming patterns",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "HuggingFaceTB/SmolLM2-135M"
            },
            {
                "type": "dataset",
                "name": "flytech/python-codes-25k"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "train_smollm_python_completion",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"PythonCompletion\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"HuggingFaceTB/SmolLM2-135M\", \"model_architecture\": \"LlamaForCausalLM\", \"formatting_template\": \"{{output}}\\n\", \"dataset_name\": \"flytech/python-codes-25k\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"4\", \"learning_rate\": \"0.0005\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"64\", \"lora_alpha\": \"128\", \"lora_dropout\": \"0.05\", \"adaptor_name\": \"python\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"flytech/python-codes-25k\\\", \\\"path\\\": \\\"flytech/python-codes-25k\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"HuggingFaceTB/SmolLM2-135M\\\", \\\"path\\\": \\\"HuggingFaceTB/SmolLM2-135M\\\"}\"}",
                "inputs_json": "{\"model_name\": \"HuggingFaceTB/SmolLM2-135M\", \"model_architecture\": \"LlamaForCausalLM\", \"dataset_name\": \"flytech/python-codes-25k\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?q=80&w=2069&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "quantize_model",
        "title": "Quantize a Model",
        "description": "Optimize your model for faster inference and reduced size using quantization tools.",
        "notes": "# Model Quantization\n\nThis recipe quantizes a model to reduce its size and improve inference speed while maintaining performance.\n\n## How to Use\nTo get a quantized model, simply go to the **Workflows** tab and run the `quantize-a-model-to-gguf` workflow.\n\n## What it does\n- Converts your model to GGUF format with q8_0 quantization\n- Reduces model size for faster loading\n- Maintains good performance with minimal accuracy loss",
        "requiredMachineArchitecture": [
            "mlx",
            "cuda"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "unsloth/Llama-3.2-1B-Instruct"
            },
            {
                "type": "plugin",
                "name": "gguf_exporter"
            }
        ],
        "tasks": [
            {
                "name": "ExportGGUFTask",
                "task_type": "EXPORT",
                "plugin": "gguf_exporter",
                "config_json": "{\"plugin_name\":\"gguf_exporter\",\"input_model_id\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_path\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_architecture\":\"LlamaForCausalLM\",\"output_model_id\":\"Llama-3.2-1B-Instruct-1752517918-q8_0.gguf\",\"output_model_architecture\":\"GGUF\",\"output_model_name\":\"Llama-3.2-1B-Instruct - GGUF - q8_0\",\"output_model_path\":\"/models/Llama-3.2-1B-Instruct-1752517918-q8_0.gguf\",\"output_filename\":\"Llama-3.2-1B-Instruct-1752517918-q8_0.gguf\",\"script_directory\":\"/plugins/gguf_exporter\",\"params\":{\"outtype\":\"q8_0\"},\"run_name\":\"ExportGGUFTask\"}",
                "inputs_json": "{\"input_model_id\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_path\":\"unsloth/Llama-3.2-1B-Instruct\",\"input_model_architecture\":\"LlamaForCausalLM\",\"plugin_name\":\"gguf_exporter\",\"plugin_architecture\":\"GGUF\"}"
            }
        ],
        "workflows": [
            {
                "name": "quantize-a-model-to-gguf",
                "config": {
                    "nodes": [
                        {
                            "type": "START",
                            "id": "c7d71d3e-98e2-4cd7-9c7a-dc749f2e5988",
                            "name": "START",
                            "out": [
                                "05b76236-0e85-4da3-a0f1-06dc36055179"
                            ],
                            "metadata": {
                                "position": {
                                    "x": -15,
                                    "y": -120
                                }
                            }
                        },
                        {
                            "name": "ExportTask",
                            "task": "ExportGGUFTask",
                            "type": "EXPORT",
                            "metadata": {
                                "position": {
                                    "x": -45,
                                    "y": 15
                                }
                            },
                            "id": "05b76236-0e85-4da3-a0f1-06dc36055179",
                            "out": []
                        }
                    ]
                }
            }
        ],
        "cardImage": "https://recipes.transformerlab.net/quantization.png"
    },
    {
        "id": "conversational_intelligence",
        "title": "Train a Model to be Conversationally Intelligent",
        "description": "Enhance a SmolLM model with advanced conversational abilities and structured response formatting using XML tags, ideal for creating a sophisticated dialogue agent.",
        "notes": "# Conversational Intelligence Training\n\n## Overview\nThis recipe develops a SmolLM model into a sophisticated conversational agent using structured dialogue formats.\n\n## Important Considerations\n- XML-based response structuring\n- Focus on natural dialogue flow\n- Balanced conversation handling\n\n## Training Tips\n- Monitor response coherence\n- Validate XML format consistency\n- Test conversation flow\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Maintain structured conversations\n- Generate well-formatted responses\n- Handle diverse dialogue scenarios",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "HuggingFaceTB/SmolLM2-135M"
            },
            {
                "type": "dataset",
                "name": "nickrosh/Evol-Instruct-Code-80k-v1"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "InstructTuning",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"InstructTuning\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"HuggingFaceTB/SmolLM2-135M\", \"model_architecture\": \"LlamaForCausalLM\", \"formatting_template\": \"<User>\\n{{instruction}}\\n</User>\\n<Assistant>\\n{{output}}\\n</Assistant>\\n\", \"dataset_name\": \"nickrosh/Evol-Instruct-Code-80k-v1\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"4\", \"learning_rate\": \"0.00003\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"4\", \"lora_alpha\": \"16\", \"lora_dropout\": \"0.05\", \"adaptor_name\": \"instruct\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"nickrosh/Evol-Instruct-Code-80k-v1\\\", \\\"path\\\": \\\"nickrosh/Evol-Instruct-Code-80k-v1\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"HuggingFaceTB/SmolLM2-135M\\\", \\\"path\\\": \\\"HuggingFaceTB/SmolLM2-135M\\\"}\"}",
                "inputs_json": "{\"model_name\": \"HuggingFaceTB/SmolLM2-135M\", \"model_architecture\": \"LlamaForCausalLM\", \"dataset_name\": \"nickrosh/Evol-Instruct-Code-80k-v1\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1573497620053-ea5300f94f21?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    },
    {
        "id": "pirate_speech",
        "title": "Train a Model to Speak like a Pirate",
        "description": "Transform a SmolLM model into a charismatic pirate conversationalist, perfect for creating engaging and entertaining interactions with a nautical twist.",
        "notes": "# Pirate Speech Transformation\n\n## Overview\nThis recipe transforms a SmolLM model into an engaging pirate-speaking assistant using specialized dialogue data.\n\n## Important Considerations\n- Maintains coherent pirate-style speech\n- Balances authenticity with understandability\n- Preserves helpful responses in pirate style\n\n## Training Tips\n- Monitor consistency of pirate speech\n- Balance entertainment with usefulness\n- Maintain appropriate language level\n\n## Expected Outcomes\nAfter training, the model should be able to:\n- Respond in consistent pirate dialect\n- Maintain helpful information delivery\n- Create engaging pirate-themed interactions",
        "requiredMachineArchitecture": [
            "cuda",
            "amd"
        ],
        "dependencies": [
            {
                "type": "model",
                "name": "HuggingFaceTB/SmolLM-135M-Instruct"
            },
            {
                "type": "dataset",
                "name": "Peyton3995/dolly-15k-mistral-pirate"
            },
            {
                "type": "plugin",
                "name": "llama_trainer"
            }
        ],
        "tasks": [
            {
                "name": "PirateSpeech",
                "task_type": "TRAIN",
                "plugin": "llama_trainer",
                "config_json": "{\"template_name\": \"PirateSpeech\", \"plugin_name\": \"llama_trainer\", \"model_name\": \"HuggingFaceTB/SmolLM-135M-Instruct\", \"model_architecture\": \"LlamaForCausalLM\", \"formatting_template\": \"<instruction>\\n{{instruction}}\\n</instruction>\\n<response>\\n{{response}}\\n</response>\", \"dataset_name\": \"Peyton3995/dolly-15k-mistral-pirate\", \"maximum_sequence_length\": \"2048\", \"batch_size\": \"4\", \"learning_rate_schedule\": \"cosine\", \"learning_rate\": \"0.01\", \"num_train_epochs\": \"1\", \"max_steps\": \"-1\", \"lora_r\": \"64\", \"lora_alpha\": \"128\", \"lora_dropout\": \"0.05\", \"adaptor_name\": \"Pirate_Speech\", \"_tlab_recipe_datasets\": \"{\\\"name\\\": \\\"Peyton3995/dolly-15k-mistral-pirate\\\", \\\"path\\\": \\\"Peyton3995/dolly-15k-mistral-pirate\\\"}\", \"_tlab_recipe_models\": \"{\\\"name\\\": \\\"HuggingFaceTB/SmolLM-135M-Instruct\\\", \\\"path\\\": \\\"HuggingFaceTB/SmolLM-135M-Instruct\\\"}\"}",
                "inputs_json": "{\"model_name\": \"HuggingFaceTB/SmolLM-135M-Instruct\", \"model_architecture\": \"LlamaForCausalLM\", \"dataset_name\": \"Peyton3995/dolly-15k-mistral-pirate\"}"
            }
        ],
        "workflows": [],
        "cardImage": "https://images.unsplash.com/photo-1652447275071-4bf852aebdc5?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
    }
]