[
  {
    "title": "Evaluation Task Example",
    "description": "A sample evaluation task that runs an evaluation script and creates reports.",
    "github_repo_url": "https://github.com/transformerlab/transformerlab-examples",
    "github_repo_dir": "demo-eval-task",
    "metadata": {
      "category": "eval",
      "modality": "text",
      "framework": ["huggingface"]
    }
  },
  {
    "title": "Basic Evaluation Metrics",
    "description": "Evaluating outputs of LLMs using basic defined metrics including regex patterns, word counts, and custom Python code.",
    "github_repo_url": "https://github.com/transformerlab/transformerlab-examples",
    "github_repo_dir": "basic-evals-metrics",
    "metadata": {
      "category": "eval",
      "modality": "text",
      "framework": [""]
    }
  },
  {
    "title": "EleutherAI LM Evaluation Harness",
    "description": "EleutherAI LM Evaluation Harness for evaluating language models on various benchmarks.",
    "github_repo_url": "https://github.com/transformerlab/transformerlab-examples",
    "github_repo_dir": "eleutherai-lm-evaluation-harness",
    "metadata": {
      "category": "eval",
      "modality": "text",
      "framework": ["eleutherai"]
    }
  }
]
