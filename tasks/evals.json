[
  {
    "title": "Evaluation Task Example",
    "description": "A sample evaluation task that runs an evaluation script and creates reports.",
    "github_repo_url": "https://github.com/transformerlab/transformerlab-examples",
    "github_repo_dir": "demo-eval-task",
    "metadata": {
      "category": "eval",
      "modality": "text",
      "framework": ["huggingface"]
    }
  },
  {
    "title": "LLM Evaluation (EleutherAI)",
    "description": "Evaluates an LLM using the EleutherAI LM Evaluation Harness. Defaults to Qwen2.5-0.5B on HellaSwag and ARC-Easy benchmarks.",
    "github_repo_url": "https://github.com/transformerlab/transformerlab-examples",
    "github_repo_dir": "llm-eval-task",
    "metadata": {
      "category": "evaluation",
      "modality": "text",
      "framework": ["eleuther"]
    }
  }
]
