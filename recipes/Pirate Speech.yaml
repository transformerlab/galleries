schemaVersion: '0.1'
metadata:
  author: ''
  version: '1.0'
  name: 'Pirate Speech'
  description: Trains SmolLM to speak like a Pirate
model:
  name: HuggingFaceTB/SmolLM-135M-Instruct
  path: HuggingFaceTB/SmolLM-135M-Instruct
datasets:
  name: Peyton3995/dolly-15k-mistral-pirate
  path: Peyton3995/dolly-15k-mistral-pirate
training:
  type: LoRA
  plugin: llama_trainer
  formatting_template: '<instruction>

    {{instruction}}

    </instruction>

    <response>

    {{response}}

    </response>'
  config_json: '{"template_name":"Pirate Speech","plugin_name":"llama_trainer","model_name":"HuggingFaceTB/SmolLM-135M-Instruct","model_architecture":"LlamaForCausalLM","formatting_template":"<instruction>\n{{instruction}}\n</instruction>\n<response>\n{{response}}\n</response>","dataset_name":"Peyton3995/dolly-15k-mistral-pirate","maximum_sequence_length":"2048","batch_size":"4","learning_rate_schedule":"cosine","learning_rate":"0.01","num_train_epochs":"1","max_steps":"-1","lora_r":"64","lora_alpha":"128","lora_dropout":"0.05","adaptor_name":"Pirate_Speech"}'
test: {}
