{
  "id": "conversational_intelligence",
  "title": "Train a Model to be Conversationally Intelligent",
  "description": "Enhance a SmolLM model with advanced conversational abilities and structured response formatting using XML tags, ideal for creating a sophisticated dialogue agent.",
  "notes": "# Train SmolLM for Conversational Intelligence\n\n## Overview\nThis recipe fine-tunes the **SmolLM2-135M** model to become a sophisticated conversational agent using structured XML dialogue formats. It uses the Evol-Instruct-Code dataset to enhance the model's ability to handle complex instructions and maintain coherent conversations.\n\nTo get started, simply go to the **Training** page and click **Queue** \u2014 the training process is ready to run.\n\n## Training Process\nThis recipe performs LoRA fine-tuning on the SmolLM model using:\n\n1. **Model**: HuggingFaceTB/SmolLM2-135M (efficient 135M parameter model)\n2. **Dataset**: Evol-Instruct-Code-80k containing diverse instruction-following examples\n3. **Training Method**: LoRA fine-tuning with XML-structured response formatting\n4. **Format**: Structured User/Assistant dialogue with XML tags\n\n## How to Use the Fine-Tuned Model\nOnce the training is complete, you can interact with the updated model directly:\n\n1. Go to the **Foundation** tab  \n2. Look for the model titled:  \n   `SmolLM2-135M_instruct`  \n3. Click **Run**  \n4. Navigate to the **Interact** tab to start chatting\n\nThe model will respond using structured formats and can handle complex conversational scenarios with improved intelligence.\n\n## Important Notes\n- This recipe uses a **very efficient 135M parameter model** for fast training and inference\n- **Structured XML formatting** helps maintain conversation coherence\n- Optimized for **CUDA and AMD GPU** architectures\n- LoRA training preserves efficiency while adding conversational capabilities\n\n## Expected Outcomes\nAfter completing this recipe, your model should be able to:\n- Maintain structured and coherent conversations\n- Follow complex instructions with appropriate responses\n- Generate well-formatted dialogue using XML structure\n- Handle diverse conversational scenarios with improved intelligence\n- Provide helpful and contextually appropriate responses",
  "requiredMachineArchitecture": [
    "cuda",
    "amd"
  ],
  "dependencies": [
    {
      "type": "model",
      "name": "HuggingFaceTB/SmolLM2-135M"
    },
    {
      "type": "dataset",
      "name": "nickrosh/Evol-Instruct-Code-80k-v1"
    },
    {
      "type": "plugin",
      "name": "llama_trainer"
    }
  ],
  "tasks": [
    {
      "name": "InstructTuning",
      "task_type": "TRAIN",
      "type": "LoRA",
      "plugin": "llama_trainer",
      "config_json": "{\"template_name\":\"InstructTuning\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"HuggingFaceTB/SmolLM2-135M\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"<User>\\n{{instruction}}\\n</User>\\n<Assistant>\\n{{output}}\\n</Assistant>\\n\",\"dataset_name\":\"nickrosh/Evol-Instruct-Code-80k-v1\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate\":\"0.00003\",\"num_train_epochs\":\"1\",\"max_steps\":\"-1\",\"lora_r\":\"4\",\"lora_alpha\":\"16\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"instruct\",\"_tlab_recipe_datasets\":{\"name\":\"nickrosh/Evol-Instruct-Code-80k-v1\",\"path\":\"nickrosh/Evol-Instruct-Code-80k-v1\"},\"_tlab_recipe_models\":{\"name\":\"HuggingFaceTB/SmolLM2-135M\",\"path\":\"HuggingFaceTB/SmolLM2-135M\"}}"
    }
  ],
  "workflows": [],
  "cardImage": "https://images.unsplash.com/photo-1573497620053-ea5300f94f21?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
}