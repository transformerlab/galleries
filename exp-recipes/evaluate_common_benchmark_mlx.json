{
  "id": "eval-common-benchmarks-non-mlx",
  "title": "Evaluate a Model on Common Benchmarks (MLX)",
  "requiredMachineArchitecture": ["mlx"],
  "description": "Performs evaluation on common benchmarks using the Eleuther AI LM Eval Harness. It evaluates the model `mlx-community/Llama-3.2-1B-Instruct-4bit` on tasks such as MMLU, Winogrande, HellaSwag, and PIQA.",
  "notes": "# Evaluate LLaMA 3.2 on Common Benchmarks (MLX)\n\n## Overview\nThis recipe evaluates the **LLaMA 3.2-1B-Instruct-4bit** model on standard language model benchmarks using the Eleuther AI LM Evaluation Harness, optimized for Apple Silicon with MLX. It tests the model's performance across multiple cognitive and reasoning tasks.\n\nTo get started, simply go to the **Workflows** page and click **Run** \u2014 the entire evaluation process is automated for you.\n\n## Workflow Breakdown\nThe workflow consists of the following automated step:\n\n1. **Benchmark Evaluation**  \n   The LLaMA 3.2-1B model is evaluated on four key benchmarks:\n   - **MMLU**: Massive Multitask Language Understanding (general knowledge)\n   - **HellaSwag**: Commonsense reasoning about everyday situations\n   - **PIQA**: Physical interaction question answering\n   - **Winogrande**: Commonsense reasoning with pronoun resolution\n\n## Understanding the Results\nAfter the evaluation completes, you'll receive detailed performance metrics for each benchmark, allowing you to:\n- Compare the model's strengths across different cognitive tasks\n- Understand how well the 4-bit quantized version performs\n- Benchmark against other models of similar size\n\n## Important Notes\n- This recipe uses the **4-bit quantized version** of LLaMA 3.2-1B for efficient evaluation on Apple Silicon\n- **MLX optimization** ensures fast evaluation on M1/M2/M3 chips\n- The evaluation uses a limited sample size (limit: 1) for quick testing\n- Results can be compared with other models evaluated on the same benchmarks\n\n## Expected Outcomes\nAfter completing this evaluation, you'll have:\n- Quantitative performance scores across four standard benchmarks\n- Insights into the model's reasoning and knowledge capabilities\n- Baseline metrics for comparing with fine-tuned or other models\n- Understanding of how quantization affects model performance",
  "dependencies": [
    {
      "type": "model",
      "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
    },
    {
      "type": "plugin",
      "name": "common-eleuther-ai-lm-eval-harness-mlx"
    }
  ],
  "tasks": [
    {
      "name": "CleanMink",
      "task_type": "EVAL",
      "plugin": "common-eleuther-ai-lm-eval-harness-mlx",
      "config_json": "{\"template_name\":\"CleanMink\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"hellaswag,mmlu,piqa,winogrande\",\"limit\":\"1\",\"run_name\":\"CleanMink\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"CleanMink\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"hellaswag,mmlu,piqa,winogrande\",\"limit\":\"1\",\"run_name\":\"CleanMink\",\"predefined_tasks\":\"\"}}"
    }
  ],
  "workflows": [
    {
      "name": "eval-on-common-benchmarks",
      "config": {
        "nodes": [
          {
            "type": "START",
            "id": "99b97abd-82de-4745-b64a-6540801261c1",
            "name": "START",
            "out": [
              "06334a95-01c4-4ece-82fc-9a107a4036e2"
            ]
          },
          {
            "name": "Eval on Harness Benchmarks",
            "task": "CleanMink",
            "type": "EVAL",
            "metadata": {
              "position": {
                "x": -75,
                "y": 105
              }
            },
            "id": "06334a95-01c4-4ece-82fc-9a107a4036e2",
            "out": []
          }
        ]
      }
    }
  ],
  "card_image": "https://images.unsplash.com/photo-1553268169-8232852a2377?q=80&w=1740&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
}