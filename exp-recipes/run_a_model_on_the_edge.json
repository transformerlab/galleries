{
  "id": "run_on_edge",
  "title": "Run a Model on the Edge",
  "description": "Execute your model on edge devices using runtime environment. Achieve real-time performance and low power consumption.",
  "notes": "# Run Qwen 2.5 Model on Edge Devices\n\n## Overview\nThis recipe demonstrates how to optimize and run the **Qwen 2.5-1.5B-Instruct** model efficiently on edge devices using FastChat server with 4-bit compression. It includes both setup and evaluation components to ensure optimal edge performance.\n\nTo get started, simply go to the **Workflows** page and click **Run** \u2014 the entire process is automated for you.\n\n## Workflow Breakdown\nThe workflow consists of the following automated steps:\n\n1. **Edge Inference Setup**  \n   The model is configured for edge deployment using FastChat server with 4-bit compression and float16 precision for optimal resource usage.\n\n2. **Performance Evaluation**  \n   The edge-optimized model is evaluated on the MMLU benchmark to assess its performance in the edge configuration.\n\n## Understanding Edge Optimization\nThis recipe optimizes the model for edge deployment by:\n- Using **4-bit compression** to reduce memory requirements\n- Implementing **float16 precision** for faster computation\n- Configuring automatic GPU layer distribution for optimal performance\n\n## Important Notes\n- This recipe optimizes for **CUDA GPU** architectures\n- **4-bit compression** significantly reduces memory usage for edge devices\n- **Float16 precision** balances speed and accuracy for real-time applications\n- The configuration automatically handles GPU layer distribution\n\n## Expected Outcomes\nAfter completing this recipe, you'll have:\n- A model optimized for edge device deployment\n- Reduced memory footprint suitable for resource-constrained environments\n- Faster inference speeds appropriate for real-time applications\n- Performance metrics showing edge optimization effectiveness\n- Understanding of edge deployment trade-offs and capabilities",
  "requiredMachineArchitecture": [
    "cuda"
  ],
  "dependencies": [
    {
      "type": "model",
      "name": "Qwen/Qwen2.5-1.5B-Instruct"
    },
    {
      "type": "plugin",
      "name": "fastchat_server"
    },
    {
      "type": "plugin",
      "name": "eleuther-ai-lm-evaluation-harness-mlx"
    }
  ],
  "tasks": [
    {
      "name": "setup_edge_inference",
      "task_type": "LOADER",
      "plugin": "fastchat_server",
      "config_json": "{\"template_name\":\"EdgeInference\",\"plugin_name\":\"fastchat_server\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"load_compressed\":\"4-bit\",\"model_dtype\":\"float16\"}"
    },
    {
      "name": "evaluate_inference",
      "task_type": "EVAL",
      "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
      "config_json": "{\"template_name\":\"EvalInference\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"tasks\":\"mmlu\",\"limit\":\"0.5\",\"run_name\":\"EvalInference\"}"
    }
  ],
  "workflows": [
    {
      "name": "Edge_Inference",
      "config": {
        "nodes": [
          {
            "id": "node_setup",
            "type": "LOADER",
            "task": "setup_edge_inference",
            "name": "Edge Setup Task",
            "out": [
              "node_eval"
            ]
          },
          {
            "id": "node_eval",
            "type": "EVAL",
            "task": "evaluate_inference",
            "name": "Evaluation Task",
            "out": []
          }
        ]
      }
    }
  ],
  "cardImage": "https://images.unsplash.com/photo-1667984550708-a6beba23cb4c?q=80&w=1932&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
}