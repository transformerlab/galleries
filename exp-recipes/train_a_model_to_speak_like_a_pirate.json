{
  "id": "pirate_speech",
  "title": "Train a Model to Speak like a Pirate",
  "description": "Transform a SmolLM model into a charismatic pirate conversationalist, perfect for creating engaging and entertaining interactions with a nautical twist.",
  "notes": "# Train SmolLM to Speak Like a Pirate\n\n## Overview\nThis recipe fine-tunes the **SmolLM-135M-Instruct** model to respond in authentic pirate speech patterns while maintaining helpful and informative responses. It uses a specialized dataset of pirate-styled conversations to create an entertaining yet functional pirate assistant.\n\nTo get started, simply go to the **Training** page and click **Queue** \u2014 the training process is ready to run.\n\n## Training Process\nThis recipe performs LoRA fine-tuning on the SmolLM model using:\n\n1. **Model**: HuggingFaceTB/SmolLM-135M-Instruct (efficient instruction-tuned model)\n2. **Dataset**: Dolly-15k-Mistral-Pirate containing conversations rewritten in pirate dialect\n3. **Training Method**: LoRA fine-tuning with structured instruction/response formatting\n4. **Style**: Authentic pirate speech patterns while preserving helpfulness\n\n## How to Use the Fine-Tuned Model\nOnce the training is complete, you can interact with the updated model directly:\n\n1. Go to the **Foundation** tab  \n2. Look for the model titled:  \n   `SmolLM-135M-Instruct_Pirate_Speech`  \n3. Click **Run**  \n4. Navigate to the **Interact** tab to start chatting\n\nAhoy! The model will now respond in pirate dialect, using expressions like \"matey,\" \"arrr,\" and \"ye\" while still providing helpful information.\n\n## Important Notes\n- This recipe uses a **very efficient 135M parameter model** for fast training and inference\n- **Pirate dialect training** maintains informativeness while adding entertainment value\n- Optimized for **CUDA and AMD GPU** architectures\n- High LoRA rank (64) ensures strong pirate speech adaptation\n\n## Expected Outcomes\nAfter completing this recipe, your model should be able to:\n- Respond consistently in authentic pirate speech patterns\n- Maintain helpful and informative responses in pirate style\n- Use appropriate pirate vocabulary and expressions\n- Balance entertainment with practical assistance\n- Create engaging pirate-themed interactions for users",
  "requiredMachineArchitecture": [
    "cuda",
    "amd"
  ],
  "dependencies": [
    {
      "type": "model",
      "name": "HuggingFaceTB/SmolLM-135M-Instruct"
    },
    {
      "type": "dataset",
      "name": "Peyton3995/dolly-15k-mistral-pirate"
    },
    {
      "type": "plugin",
      "name": "llama_trainer"
    }
  ],
  "tasks": [
    {
      "name": "PirateSpeech",
      "task_type": "TRAIN",
      "type": "LoRA",
      "plugin": "llama_trainer",
      "config_json": "{\"template_name\":\"PirateSpeech\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"HuggingFaceTB/SmolLM-135M-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"<instruction>\\n{{instruction}}\\n</instruction>\\n<response>\\n{{response}}\\n</response>\",\"dataset_name\":\"Peyton3995/dolly-15k-mistral-pirate\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate_schedule\":\"cosine\",\"learning_rate\":\"0.01\",\"num_train_epochs\":\"1\",\"max_steps\":\"-1\",\"lora_r\":\"64\",\"lora_alpha\":\"128\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"Pirate_Speech\",\"_tlab_recipe_datasets\":{\"name\":\"Peyton3995/dolly-15k-mistral-pirate\",\"path\":\"Peyton3995/dolly-15k-mistral-pirate\"},\"_tlab_recipe_models\":{\"name\":\"HuggingFaceTB/SmolLM-135M-Instruct\",\"path\":\"HuggingFaceTB/SmolLM-135M-Instruct\"}}"
    }
  ],
  "workflows": [],
  "cardImage": "https://images.unsplash.com/photo-1652447275071-4bf852aebdc5?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
}