{
  "id": "ml_qa_mlx",
  "title": "Machine Learning Q&A on MLX",
  "description": "Fine-tune a Gemma 2 model to become an expert in answering machine learning questions, optimized for Apple Silicon using MLX framework.",
  "notes": "# Fine-Tune Gemma 2 on Machine Learning Q&A (MLX)\n\n## Overview\nThis recipe fine-tunes the **Gemma 2-2B-IT** model to specialize in answering questions about machine learning, optimized for Apple Silicon using the MLX framework. It uses a high-quality dataset of ML-focused conversations to create an expert ML assistant.\n\nTo get started, simply go to the **Training** page and click **Queue** \u2014 the training process is ready to run.\n\n## Training Process\nThis recipe performs LoRA fine-tuning on the Gemma 2 model using:\n\n1. **Model**: google/gemma-2-2b-it (2B parameter instruction-tuned model)\n2. **Dataset**: Machine Learning Q&A Collection containing diverse ML questions and answers\n3. **Training Method**: LoRA (Low-Rank Adaptation) for efficient fine-tuning\n4. **Optimization**: MLX framework for optimal performance on Apple Silicon\n\n## How to Use the Fine-Tuned Model\nOnce the training is complete, you can interact with the updated model directly:\n\n1. Go to the **Foundation** tab  \n2. Look for the model titled:  \n   `google/gemma-2-2b-it_ml-qa`  \n3. Click **Run**  \n4. Navigate to the **Interact** tab to start chatting\n\nYou can now test the model on real-time questions about topics like supervised learning, neural networks, optimization algorithms, model evaluation, and more.\n\n## Important Notes\n- This recipe uses **MLX optimization** for Apple Silicon (M1/M2/M3) chips\n- LoRA training allows efficient fine-tuning while preserving the base model's capabilities\n- The model specializes in machine learning concepts while maintaining general conversational abilities\n\n## Expected Outcomes\nAfter completing this recipe, your model should be able to:\n- Provide detailed explanations of ML concepts and algorithms\n- Answer both theoretical and practical machine learning questions\n- Offer guidance on ML implementation and best practices\n- Maintain clarity while delivering technically accurate information",
  "requiredMachineArchitecture": [
    "mlx"
  ],
  "dependencies": [
    {
      "type": "model",
      "name": "google/gemma-2-2b-it"
    },
    {
      "type": "dataset",
      "name": "win-wang/Machine_Learning_QA_Collection"
    },
    {
      "type": "plugin",
      "name": "mlx_lora_trainer"
    }
  ],
  "tasks": [
    {
      "name": "train_gemma_2_ml_qa",
      "task_type": "TRAIN",
      "type": "LoRA",
      "plugin": "mlx_lora_trainer",
      "config_json": "{\"template_name\":\"MachineLearningQnA-MLX\",\"plugin_name\":\"mlx_lora_trainer\",\"model_name\":\"google/gemma-2-2b-it\",\"model_architecture\":\"Gemma2ForCausalLM\",\"formatting_template\":\"{{text}}\",\"dataset_name\":\"win-wang/Machine_Learning_QA_Collection\",\"lora_layers\":\"8\",\"batch_size\":\"4\",\"learning_rate\":\"0.0001\",\"lora_rank\":\"8\",\"lora_alpha\":\"160\",\"iters\":\"200\",\"steps_per_report\":\"10\",\"steps_per_eval\":\"50\",\"save_every\":\"50\",\"adaptor_name\":\"ml-qa\",\"_tlab_recipe_datasets\":{\"name\":\"Machine Learning QA Collection\",\"path\":\"win-wang/Machine_Learning_QA_Collection\"},\"_tlab_recipe_models\":{\"name\":\"google/gemma-2-2b-it\",\"path\":\"google/gemma-2-2b-it\"}}"
    }
  ],
  "workflows": [],
  "cardImage": "https://images.unsplash.com/photo-1557562645-4eee56b29bc1?q=80&w=1935&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
}
