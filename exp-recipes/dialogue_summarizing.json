{
  "id": "dialogue_summarizing",
  "title": "Dialogue Summarizing",
  "description": "Fine-tune a TinyLlama model to create concise, accurate summaries of conversations and dialogues. Perfect for chat logs, meeting transcripts, and customer service interactions.",
  "notes": "# Train TinyLlama for Dialogue Summarization\n\n## Overview\nThis recipe fine-tunes the **TinyLlama-1.1B-Chat-v1.0** model to excel at creating concise, accurate summaries of conversations and dialogues. It uses the SAMSum dataset to train the model on diverse conversation formats, making it perfect for summarizing chat logs, meeting transcripts, and customer service interactions.\n\nTo get started, simply go to the **Training** page and click **Queue** \u2014 the training process is ready to run.\n\n## Training Process\nThis recipe performs LoRA fine-tuning on the TinyLlama model using:\n\n1. **Model**: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (efficient 1.1B parameter chat model)\n2. **Dataset**: SAMSum containing diverse conversation summaries and dialogue formats\n3. **Training Method**: LoRA fine-tuning with structured instruction/response formatting\n4. **Format**: \"Instruction: Summarize the Following\" format for clear task definition\n\n## How to Use the Fine-Tuned Model\nOnce the training is complete, you can interact with the updated model directly:\n\n1. Go to the **Foundation** tab  \n2. Look for the model titled:  \n   `TinyLlama-1.1B-Chat-v1.0_Summarizer`  \n3. Click **Run**  \n4. Navigate to the **Interact** tab to start chatting\n\nYou can now provide dialogues and conversations for the model to summarize with high accuracy and appropriate brevity.\n\n## Important Notes\n- This recipe uses **TinyLlama** optimized for efficiency while maintaining quality performance\n- Optimized for **CUDA and AMD GPU** architectures\n- **LoRA fine-tuning** provides memory-efficient training with excellent results\n- The SAMSum dataset contains diverse conversation styles and formats\n\n## Expected Outcomes\nAfter completing this recipe, your model should be able to:\n- Generate concise, accurate summaries of conversations and dialogues\n- Maintain key information and essential context from original conversations\n- Handle various dialogue formats including casual chats, formal meetings, and structured conversations\n- Balance brevity with comprehensive information retention\n- Provide professional-quality summaries suitable for business and personal use",
  "requiredMachineArchitecture": [
    "cuda",
    "amd"
  ],
  "dependencies": [
    {
      "type": "model",
      "name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    },
    {
      "type": "dataset",
      "name": "knkarthick/samsum"
    },
    {
      "type": "plugin",
      "name": "llama_trainer"
    }
  ],
  "tasks": [
    {
      "name": "train_tinyllama_summarizer",
      "task_type": "TRAIN",
      "type": "LoRA",
      "plugin": "llama_trainer",
      "formatting_template": "Instruction: Summarize the Following\nPrompt: {{dialogue}}\nGeneration: {{summary}}",
      "config_json": "{\"template_name\":\"DialogueSummarizing\",\"plugin_name\":\"llama_trainer\",\"model_name\":\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\"model_architecture\":\"LlamaForCausalLM\",\"formatting_template\":\"Instruction: Summarize the Following\\nPrompt: {{dialogue}}\\nGeneration: {{summary}}\",\"dataset_name\":\"knkarthick/samsum\",\"maximum_sequence_length\":\"2048\",\"batch_size\":\"4\",\"learning_rate\":\"0.00005\",\"num_train_epochs\":\"1\",\"max_steps\":\"-1\",\"lora_r\":\"32\",\"lora_alpha\":\"64\",\"lora_dropout\":\"0.05\",\"adaptor_name\":\"Summarizer\",\"_tlab_recipe_datasets\":{\"name\":\"knkarthick/samsum\",\"path\":\"knkarthick/samsum\"},\"_tlab_recipe_models\":{\"name\":\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\"path\":\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"}}"
    }
  ],
  "workflows": [],
  "cardImage": "https://images.unsplash.com/photo-1590650046871-92c887180603?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
}