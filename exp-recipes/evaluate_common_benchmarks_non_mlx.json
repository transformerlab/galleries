{
  "id": "eval-common-benchmarks-non-mlx",
  "title": "Evaluate a Model on Common Benchmarks",
  "requiredMachineArchitecture": ["cuda", "amd"],
  "description": "Performs evaluation on common benchmarks using the Eleuther AI LM Eval Harness. It evaluates the model `unsloth/Llama-3.2-1B-Instruct` on tasks such as MMLU, Winogrande, HellaSwag, and PIQA.",
  "notes": "# Model Evaluation on Common Benchmarks\n\n To run this recipe, go to **Evaluate** in the sidebar and click on Queue\n\n## Overview\nThis recipe evaluates a Llama 3.2 model on common benchmarks using the Eleuther AI LM Eval Harness.\n\n## Expected Outcome\nAfter evaluation, you can view:\n- Detailed performance reports\n- Comparative charts and metrics\n- Benchmark scores for MMLU, Winogrande, HellaSwag, and PIQA",
  "dependencies": [
    {
      "type": "model",
      "name": "unsloth/Llama-3.2-1B-Instruct"
    },
    {
      "type": "plugin",
      "name": "common-eleuther-ai-lm-eval-harness"
    }
  ],
  "tasks": [
    {
      "name": "KindMoose",
      "task_type": "EVAL",
      "plugin": "common-eleuther-ai-lm-eval-harness",
      "config_json": "{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\"}}",
      "inputs_json": "{}"
    }
  ],
  "workflows": [],
  "cardImage": "https://recipes.transformerlab.net/radialchart.png"
}
