{
  "id": "eval-common-benchmarks-non-mlx",
  "title": "Evaluate a Model on Common Benchmarks (Non-MLX)",
  "requiredMachineArchitecture": ["cuda", "amd"],
  "description": "Performs evaluation on common benchmarks using the Eleuther AI LM Eval Harness. It evaluates the model `unsloth/Llama-3.2-1B-Instruct` on tasks such as MMLU, Winogrande, HellaSwag, and PIQA.",
  "notes": "# Evaluate LLaMA 3.2 on Common Benchmarks (Non-MLX)\n\n## Overview\nThis recipe evaluates the **LLaMA 3.2-1B-Instruct** model on standard language model benchmarks using the Eleuther AI LM Evaluation Harness, optimized for CUDA and AMD GPUs. It tests the model's performance across multiple cognitive and reasoning tasks.\n\nTo get started, simply go to the **Workflows** page and click **Run** \u2014 the entire evaluation process is automated for you.\n\n## Workflow Breakdown\nThe workflow consists of the following automated step:\n\n1. **Benchmark Evaluation**  \n   The LLaMA 3.2-1B model is evaluated on four key benchmarks:\n   - **MMLU**: Massive Multitask Language Understanding (general knowledge)\n   - **HellaSwag**: Commonsense reasoning about everyday situations\n   - **PIQA**: Physical interaction question answering\n   - **Winogrande**: Commonsense reasoning with pronoun resolution\n\n## Understanding the Results\nAfter the evaluation completes, you'll receive detailed performance metrics for each benchmark, allowing you to:\n- Compare the model's strengths across different cognitive tasks\n- Understand the full-precision model's capabilities\n- Benchmark against other models of similar size\n\n## Important Notes\n- This recipe uses the **full-precision version** of LLaMA 3.2-1B for maximum accuracy\n- Optimized for **CUDA and AMD GPU** architectures\n- The evaluation uses a limited sample size (limit: 1) for quick testing\n- Results can be compared with quantized versions to understand precision trade-offs\n\n## Expected Outcomes\nAfter completing this evaluation, you'll have:\n- Quantitative performance scores across four standard benchmarks\n- Insights into the model's reasoning and knowledge capabilities\n- Baseline metrics for comparing with fine-tuned or quantized models\n- Understanding of full-precision model performance",
  "dependencies": [
    {
      "type": "model",
      "name": "unsloth/Llama-3.2-1B-Instruct"
    },
    {
      "type": "plugin",
      "name": "common-eleuther-ai-lm-eval-harness"
    }
  ],
  "tasks": [
    {
      "name": "KindMoose",
      "task_type": "EVAL",
      "plugin": "common-eleuther-ai-lm-eval-harness",
      "config_json": "{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\",\"script_parameters\":{\"template_name\":\"KindMoose\",\"plugin_name\":\"common-eleuther-ai-lm-eval-harness\",\"model_name\":\"unsloth/Llama-3.2-1B-Instruct\",\"model_architecture\":\"LlamaForCausalLM\",\"tasks\":\"mmlu,winogrande,hellaswag,piqa\",\"limit\":\"1\",\"run_name\":\"KindMoose\",\"predefined_tasks\":\"\"}}"
    }
  ],
  "workflows": [
    {
      "name": "evaluate-on-common-benchmarks",
      "config": {
        "nodes": [
          {
            "type": "START",
            "id": "bc9dc3a4-afba-4956-a55f-bd51e96da24f",
            "name": "START",
            "out": [
              "009309fa-1ed5-42bc-be5d-b84e32772bf1"
            ]
          },
          {
            "name": "EVAL HARNESS",
            "task": "KindMoose",
            "type": "EVAL",
            "metadata": {
              "position": {
                "x": -60,
                "y": 75
              }
            },
            "id": "009309fa-1ed5-42bc-be5d-b84e32772bf1",
            "out": []
          }
        ]
      }
    }
  ],
  "card_image": "https://images.unsplash.com/photo-1589595427524-2ddaf2d43fc9?q=80&w=1744&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
}