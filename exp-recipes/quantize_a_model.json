{
  "id": "quantize_model",
  "title": "Quantize a Model",
  "description": "Optimize your model for faster inference and reduced size using quantization tools.",
  "notes": "# Quantize Qwen 2.5 Model for Efficient Inference\n\n## Overview\nThis recipe demonstrates how to quantize the **Qwen 2.5-1.5B-Instruct** model to reduce its size and improve inference speed while maintaining performance. It includes both quantization and evaluation components to assess the impact of compression.\n\nTo get started, simply go to the **Workflows** page and click **Run** \u2014 the entire process is automated for you.\n\n## Workflow Breakdown\nThe workflow consists of the following automated steps:\n\n1. **Model Quantization**  \n   The Qwen 2.5 model is quantized using 4-bit precision (q4_k_m format) through the llama.cpp framework, significantly reducing model size while preserving quality.\n\n2. **Post-Quantization Evaluation**  \n   The quantized model is evaluated on the MMLU benchmark to assess how quantization affects performance compared to the original model.\n\n## Understanding the Results\nAfter quantization completes, you'll have:\n- A significantly smaller model file (typically 4x reduction in size)\n- Faster inference speeds on supported hardware\n- Performance metrics showing the quality trade-offs\n\n## Important Notes\n- This recipe uses **4-bit quantization (q4_k_m)** for optimal balance of size and quality\n- Compatible with **MLX and CUDA** architectures\n- The quantized model maintains most of the original model's capabilities\n- Evaluation helps you understand the performance impact of compression\n\n## Expected Outcomes\nAfter completing this recipe, you'll have:\n- A quantized model with ~75% smaller file size\n- Significantly faster inference speeds\n- Detailed performance metrics comparing original vs. quantized versions\n- Understanding of quantization trade-offs for deployment decisions\n- A deployable model optimized for resource-constrained environments",
  "requiredMachineArchitecture": [
    "mlx",
    "cuda"
  ],
  "dependencies": [
    {
      "type": "model",
      "name": "Qwen/Qwen2.5-1.5B-Instruct"
    },
    {
      "type": "plugin",
      "name": "llama_cpp_server"
    },
    {
      "type": "plugin",
      "name": "eleuther-ai-lm-evaluation-harness-mlx"
    }
  ],
  "tasks": [
    {
      "name": "quantize_model",
      "task_type": "EXPORT",
      "plugin": "llama_cpp_server",
      "config_json": "{\"template_name\":\"Quantization\",\"plugin_name\":\"llama_cpp_server\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"quantization_type\":\"q4_k_m\",\"n_gpu_layers\":\"auto\"}"
    },
    {
      "name": "evaluate_quantized",
      "task_type": "EVAL",
      "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
      "config_json": "{\"template_name\":\"EvalQuantized\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"model_architecture\":\"Qwen2ForCausalLM\",\"tasks\":\"mmlu\",\"limit\":\"0.5\",\"run_name\":\"EvalQuantized\"}"
    }
  ],
  "workflows": [
    {
      "name": "Quantize_and_Evaluate",
      "config": {
        "nodes": [
          {
            "id": "node_quantize",
            "type": "EXPORT",
            "task": "quantize_model",
            "name": "Quantization Task",
            "out": [
              "node_eval"
            ]
          },
          {
            "id": "node_eval",
            "type": "EVAL",
            "task": "evaluate_quantized",
            "name": "Evaluation Task",
            "out": []
          }
        ]
      }
    }
  ],
  "cardImage": "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
}