{
  "id": "evaluate_model",
  "title": "Evaluate a Model",
  "description": "Assess the performance of your model using Eleuther Labs AI Evaluation Harness. Gain insights into accuracy and reliability.",
  "notes": "# Comprehensive Model Evaluation with MLX\n\n## Overview\nThis recipe provides a comprehensive evaluation of the **LLaMA 3.2-1B-Instruct-4bit** model using the Eleuther AI Evaluation Harness, optimized for Apple Silicon with MLX. It tests the model across multiple benchmarks to assess its knowledge, reasoning, and truthfulness capabilities.\n\nTo get started, simply go to the **Workflows** page and click **Run** \u2014 the entire evaluation process is automated for you.\n\n## Workflow Breakdown\nThe workflow consists of the following automated steps:\n\n1. **MMLU Evaluation**  \n   The model is evaluated on the Massive Multitask Language Understanding benchmark, testing its knowledge across 57 academic subjects including mathematics, history, science, and law.\n\n2. **TruthfulQA Evaluation**  \n   The model is assessed on its ability to provide truthful answers and avoid generating false information, testing for factual accuracy and reliability.\n\n## Understanding the Results\nAfter the evaluation completes, you'll receive comprehensive metrics for:\n- **MMLU Performance**: Scores across different academic domains and overall knowledge retention\n- **TruthfulQA Performance**: Assessment of factual accuracy and tendency to generate truthful responses\n- **Comparative Analysis**: Understanding how the 4-bit quantized model performs relative to full-precision versions\n\n## Important Notes\n- This recipe uses **MLX optimization** for Apple Silicon (M1/M2/M3) chips\n- Evaluates the **4-bit quantized version** to assess quantization impact on performance\n- Uses **full evaluation** (limit: 1.0) for comprehensive and accurate results\n- Results provide baseline metrics for comparing with fine-tuned models\n\n## Expected Outcomes\nAfter completing this evaluation, you'll have:\n- Detailed performance scores across knowledge and truthfulness benchmarks\n- Understanding of the model's strengths and potential weaknesses\n- Baseline metrics for comparison with other models or fine-tuned versions\n- Insights into how quantization affects model capabilities\n- Data-driven understanding of the model's suitability for specific tasks",
  "requiredMachineArchitecture": [
    "mlx"
  ],
  "dependencies": [
    {
      "type": "model",
      "name": "mlx-community/Llama-3.2-1B-Instruct-4bit"
    },
    {
      "type": "dataset",
      "name": "spencer/samsum_reformat"
    },
    {
      "type": "plugin",
      "name": "eleuther-ai-lm-evaluation-harness-mlx"
    }
  ],
  "tasks": [
    {
      "name": "evaluate_model_mmlu",
      "task_type": "EVAL",
      "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
      "config_json": "{\"template_name\":\"EvalMMLU\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"mmlu\",\"limit\":\"1.0\",\"run_name\":\"EvalMMLU\"}"
    },
    {
      "name": "evaluate_model_truthfulqa",
      "task_type": "EVAL",
      "plugin": "eleuther-ai-lm-evaluation-harness-mlx",
      "config_json": "{\"template_name\":\"EvalTruthfulQA\",\"plugin_name\":\"eleuther-ai-lm-evaluation-harness-mlx\",\"model_name\":\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\"model_architecture\":\"MLX\",\"tasks\":\"truthfulqa\",\"limit\":\"1.0\",\"run_name\":\"EvalTruthfulQA\"}"
    }
  ],
  "workflows": [
    {
      "name": "Comprehensive_Evaluation",
      "config": {
        "nodes": [
          {
            "id": "node_eval_mmlu",
            "type": "EVAL",
            "task": "evaluate_model_mmlu",
            "name": "MMLU Evaluation",
            "out": [
              "node_eval_truthfulqa"
            ]
          },
          {
            "id": "node_eval_truthfulqa",
            "type": "EVAL",
            "task": "evaluate_model_truthfulqa",
            "name": "TruthfulQA Evaluation",
            "out": []
          }
        ]
      }
    }
  ],
  "cardImage": "https://images.unsplash.com/photo-1606326608606-aa0b62935f2b?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
}