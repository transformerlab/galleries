[
    {
        "uniqueID": "vicuna-7b-v1.5",
        "name": "Vicuna 7b",
        "description": "Vicuna is a state-of-the-art chat assistant developed by LMSYS, leveraging the transformative power of an auto-regressive language model based on the transformer architecture. Trained via fine-tuning the Llama 2 model, Vicuna is used primarily for research in large language models and chatbots, catering to the needs of AI enthusiasts, machine learning researchers, and hobbyists. The model operates under the Llama 2 CLA. The latest version, Vicuna v1.5 (16k), has been fine-tuned using supervised instruction and linear RoPE scaling, with training data comprising around 125K conversations collected from ShareGPT.com, packed into sequences of 16K tokens each. A comprehensive explanation of the training details can be found in the appendix to the linked paper titled \"Training Details of Vicuna Models.\"",
        "parameters": "13B",
        "context": "4k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "lmsys/vicuna-7b-v1.5",
        "transformers_version": "4.31.0",
        "license": "Llama 2 CLA",
        "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
        "size_of_model_in_mb": 12247.3,
        "author": {
            "name": "LMSYS Org",
            "url": "https://lmsys.org/",
            "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
            "downloadUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
            "paperUrl": "https://arxiv.org/abs/2306.05685"
        }
    },
    {
        "uniqueID": "vicuna-13b-v1.5",
        "name": "Vicuna 13b",
        "description": "Vicuna is a state-of-the-art chat assistant developed by LMSYS, leveraging the transformative power of an auto-regressive language model based on the transformer architecture. Trained via fine-tuning the Llama 2 model, Vicuna is used primarily for research in large language models and chatbots, catering to the needs of AI enthusiasts, machine learning researchers, and hobbyists. The model operates under the Llama 2 CLA. The latest version, Vicuna v1.5 (16k), has been fine-tuned using supervised instruction and linear RoPE scaling, with training data comprising around 125K conversations collected from ShareGPT.com, packed into sequences of 16K tokens each. A comprehensive explanation of the training details can be found in the appendix to the linked paper titled \"Training Details of Vicuna Models.\"",
        "parameters": "13B",
        "context": "4k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "lmsys/vicuna-13b-v1.5",
        "transformers_version": "4.31.0",
        "license": "Llama 2 CLA",
        "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
        "author": {
            "name": "LMSYS Org",
            "url": "https://lmsys.org/",
            "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/lmsys/vicuna-13b-v1.5",
            "downloadUrl": "https://huggingface.co/lmsys/vicuna-13b-v1.5",
            "paperUrl": "https://arxiv.org/abs/2306.05685"
        }
    },
    {
        "uniqueID": "vicuna-7b-v1.5-16k",
        "name": "Vicuna 7b - 16k",
        "description": "Vicuna is a state-of-the-art chat assistant developed by LMSYS, leveraging the transformative power of an auto-regressive language model based on the transformer architecture. Trained via fine-tuning the Llama 2 model, Vicuna is used primarily for research in large language models and chatbots, catering to the needs of AI enthusiasts, machine learning researchers, and hobbyists. The model operates under the Llama 2 CLA. The latest version, Vicuna v1.5 (16k), has been fine-tuned using supervised instruction and linear RoPE scaling, with training data comprising around 125K conversations collected from ShareGPT.com, packed into sequences of 16K tokens each. A comprehensive explanation of the training details can be found in the appendix to the linked paper titled \"Training Details of Vicuna Models.\"",
        "parameters": "7B",
        "context": "16k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "lmsys/vicuna-7b-v1.5-16k",
        "transformers_version": "4.31.0",
        "license": "Llama 2 CLA",
        "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
        "size_of_model_in_mb": 8737.3,
        "author": {
            "name": "LMSYS Org",
            "url": "https://lmsys.org/",
            "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5-16k",
            "downloadUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5-16k",
            "paperUrl": "https://arxiv.org/abs/2306.05685"
        }
    }
]