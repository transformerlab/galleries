[
    {
        "uniqueID": "bigcode/starcoder",
        "name": "StarCoder",
        "description": "The StarCoder models are 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2), with opt-out requests excluded. The model uses Multi Query Attention, a context window of 8192 tokens, and was trained using the Fill-in-the-Middle objective on 1 trillion tokens.",
        "archived": true,
        "parameters": "7B",
        "tags": [
          "Coding"
        ],
        "context": "4k",
        "architecture": "GPTBigCodeForCausalLM",
        "formats": [
          "Pytorch"
        ],
        "huggingface_repo": "bigcode/starcoder",
        "transformers_version": "4.28.1",
        "gated": "auto",
        "license": "bigcode-openrail-m",
        "logo": "https://www.bigcode-project.org/flow.png",
        "size_of_model_in_mb": 120695.78,
        "author": {
            "name": "Big Code",
            "url": "https://www.bigcode-project.org/",
            "blurb": "BigCode is an open scientific collaboration working on the responsible development and use of large language models for code"
        },
        "resources": {
            "canonicalUrl": "https://www.bigcode-project.org/",
            "downloadUrl": "https://huggingface.co/bigcode/starcoder",
            "paperUrl": "?"
        },
        "model_group": "starcoder"
    }
]