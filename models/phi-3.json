[
  {
    "uniqueID": "microsoft/Phi-3-mini-128k-instruct",
    "name": "Phi 3 Mini 128K Instruct",
    "description": "The Phi-3-Mini-128K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support. The model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures. When assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.",
    "parameters": "3.8B",
    "context": "128K",
    "architecture": "Phi3ForCausalLM",
    "formats": [
      "Safetensors"
    ],
    "huggingface_repo": "microsoft/Phi-3-mini-128k-instruct",
    "transformers_version": "4.39.3",
    "gated": "auto",
    "license": "MIT",
    "logo": "https://blogs.microsoft.com/wp-content/uploads/prod/2012/08/8867.Microsoft_5F00_Logo_2D00_for_2D00_screen.jpg",
    "size_of_model_in_mb": 7290.59,
    "author": {
      "name": "Microsoft",
      "url": "https://huggingface.co/papers/2404.14219",
      "blurb": ""
    },
    "resources": {
      "canonicalUrl": "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct",
      "downloadUrl": "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct",
      "paperUrl": "https://huggingface.co/papers/2404.14219"
    }
  },
  {
    "uniqueID": "microsoft/Phi-3-mini-4k-instruct",
    "name": "Phi 3 Mini 4K Instruct",
    "description": "The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support. The model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures. When assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.",
    "parameters": "3.8B",
    "context": "4K",
    "architecture": "Phi3ForCausalLM",
    "formats": [
      "Safetensors"
    ],
    "huggingface_repo": "microsoft/Phi-3-mini-4k-instruct",
    "transformers_version": "4.39.3",
    "gated": "auto",
    "license": "MIT",
    "logo": "https://blogs.microsoft.com/wp-content/uploads/prod/2012/08/8867.Microsoft_5F00_Logo_2D00_for_2D00_screen.jpg",
    "size_of_model_in_mb": 7290.58,
    "author": {
      "name": "Microsoft",
      "url": "https://huggingface.co/papers/2404.14219",
      "blurb": ""
    },
    "resources": {
      "canonicalUrl": "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct",
      "downloadUrl": "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct",
      "paperUrl": "https://huggingface.co/papers/2404.14219"
    }
  }
]
