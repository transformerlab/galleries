[
    {
        "uniqueID": "Nous-Hermes-13b",
        "name": "Nous Hermes 13b",
        "description": "Extremely good model. Instruction based. Gives long responses. Curated with 300,000 uncensored instructions. Trained by Nous Research. Cannot be used commercially",
        "parameters": "13B",
        "context": "?",
        "architecture": "LlamaForCausalLM",
        "formats": [
            "PyTorch"
        ],
        "huggingface_repo": "NousResearch/Nous-Hermes-13b",
        "transformers_version": "4.29.2",
        "gated": false,
        "license": "GPL",
        "logo": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/ngyQKdhaykfC8lN6Jfqlz.png?w=200&h=200&f=face",
        "size_of_model_in_mb": 24826.4,
        "author": {
            "name": "Nous Research",
            "url": "https://nousresearch.com/",
            "blurb": "We are dedicated to advancing the field of natural language processing, in collaboration with the open-source community, through bleeding-edge research and a commitment to symbiotic development."
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/NousResearch/Nous-Hermes-13b",
            "downloadUrl": "https://huggingface.co/NousResearch/Nous-Hermes-13b",
            "paperUrl": "?"
        },
        "group": "nous-hermes"
    },
    {
        "uniqueID": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
        "name": "Nous Hermes 2 - Mixtral 8x7B - DPO",
        "description": "Nous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research model trained over the Mixtral 8x7B MoE LLM.",
        "parameters": "7B",
        "context": "4k",
        "architecture": "MixtralForCausalLM",
        "formats": [
            "Safetensors"
        ],
        "huggingface_repo": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
        "transformers_version": "4.37.0.dev0",
        "gated": false,
        "license": "Apache 2.0",
        "logo": "https://pbs.twimg.com/profile_images/1722061115453272064/dydqIH88_400x400.jpg",
        "size_of_model_in_mb": 89080.94,
        "author": {
            "name": "Nous Research",
            "url": "https://nousresearch.com/",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
            "downloadUrl": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
            "paperUrl": "?"
        },
        "group": "nous-hermes"
    }
]