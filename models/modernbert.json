[
  {
    "uniqueID": "dllm-collection/ModernBERT-base-chat-v0",
    "name": "ModernBERT-base-chat-v0",
    "description": "ModernBERT-base-chat-v0 is a generative variant of ModernBERT-base, finetuned using the dLLM framework. This model demonstrates that BERT can serve as a diffusion-based chatbot trained purely with supervised instruction data — no continual generative pretraining required.",
    "added": "2025-11-20",
    "tags": ["Diffusion", "Text Generation", "Masked Language Modeling"],
    "parameters": "0.2B",
    "context": "8192",
    "architecture": "ModernBertForMaskedLM",
    "formats": [
      "Safetensors"
    ],
    "huggingface_repo": "dllm-collection/ModernBERT-base-chat-v0",
    "transformers_version": "4.57.0",
    "gated": false,
    "license": "apache-2.0",
    "logo": "https://www.gravatar.com/avatar/0cd0099e911d0ac6315a4ea9675343a2?d=retro&size=100",
    "size_of_model_in_mb": 362.6850233078003,
    "author": {
      "name": "dllm-collection",
      "url": "https://huggingface.co/dllm-collection/ModernBERT-base-chat-v0",
      "blurb": ""
    },
    "resources": {
      "canonicalUrl": "https://huggingface.co/dllm-collection/ModernBERT-base-chat-v0",
      "downloadUrl": "https://huggingface.co/dllm-collection/ModernBERT-base-chat-v0",
      "paperUrl": "?"
    },
    "model_group": "Text Diffusion"
  },
  {
    "uniqueID": "dllm-collection/ModernBERT-large-chat-v0",
    "name": "ModernBERT-large-chat-v0",
    "description": "ModernBERT-large-chat-v0 is a generative variant of ModernBERT-large, finetuned using the dLLM framework. This model demonstrates that BERT can serve as a diffusion-based chatbot trained purely with supervised instruction data — no continual generative pretraining required.",
    "added": "2025-11-20",
    "tags": ["Diffusion", "Text Generation", "Masked Language Modeling"],
    "parameters": "0.4B",
    "context": "8192",
    "architecture": "ModernBertForMaskedLM",
    "formats": [
      "Safetensors"
    ],
    "huggingface_repo": "dllm-collection/ModernBERT-large-chat-v0",
    "transformers_version": "4.57.0",
    "gated": false,
    "license": "apache-2.0",
    "logo": "https://www.gravatar.com/avatar/0cd0099e911d0ac6315a4ea9675343a2?d=retro&size=100",
    "size_of_model_in_mb": 856.9222221374512,
    "author": {
      "name": "dllm-collection",
      "url": "https://huggingface.co/dllm-collection/ModernBERT-large-chat-v0",
      "blurb": ""
    },
    "resources": {
      "canonicalUrl": "https://huggingface.co/dllm-collection/ModernBERT-large-chat-v0",
      "downloadUrl": "https://huggingface.co/dllm-collection/ModernBERT-large-chat-v0",
      "paperUrl": "?"
    },
    "model_group": "Text Diffusion"
  },
  {
    "uniqueID": "answerdotai/ModernBERT-base",
    "name": "ModernBERT-base",
    "description": "ModernBERT is a modernized bidirectional encoder-only Transformer model (BERT-style) pre-trained on 2 trillion tokens of English and code data with a native context length of up to 8,192 tokens. ",
    "added": "2025-11-20",
    "tags": ["Diffusion", "Text Generation", "Masked Language Modeling"],
    "parameters": "0.1B",
    "context": "8192",
    "architecture": "ModernBertForMaskedLM",
    "formats": [
      "Safetensors"
    ],
    "huggingface_repo": "answerdotai/ModernBERT-base",
    "transformers_version": "4.47.0.dev0",
    "gated": false,
    "license": "apache-2.0",
    "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/623e19ff8972a8c030af5277/KTqmARlLjdqMClfLWA-A1.png",
    "size_of_model_in_mb": 1143.889181137085,
    "author": {
      "name": "answerdotai",
      "url": "https://huggingface.co/answerdotai/ModernBERT-base",
      "blurb": ""
    },
    "resources": {
      "canonicalUrl": "https://huggingface.co/answerdotai/ModernBERT-base",
      "downloadUrl": "https://huggingface.co/answerdotai/ModernBERT-base",
      "paperUrl": "?"
    },
    "model_group": "Text Diffusion"
  },
  {
    "uniqueID": "answerdotai/ModernBERT-large",
    "name": "ModernBERT-large",
    "description": "ModernBERT is a modernized bidirectional encoder-only Transformer model (BERT-style) pre-trained on 2 trillion tokens of English and code data with a native context length of up to 8,192 tokens.",
    "added": "2025-11-20",
    "tags": ["Diffusion", "Text Generation", "Masked Language Modeling"],
    "parameters": "0.4B",
    "context": "8192",
    "architecture": "ModernBertForMaskedLM",
    "formats": [
      "Safetensors"
    ],
    "huggingface_repo": "answerdotai/ModernBERT-large",
    "transformers_version": "4.47.0.dev0",
    "gated": false,
    "license": "apache-2.0",
    "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/623e19ff8972a8c030af5277/KTqmARlLjdqMClfLWA-A1.png",
    "size_of_model_in_mb": 3022.462546348572,
    "author": {
      "name": "answerdotai",
      "url": "https://huggingface.co/answerdotai/ModernBERT-large",
      "blurb": ""
    },
    "resources": {
      "canonicalUrl": "https://huggingface.co/answerdotai/ModernBERT-large",
      "downloadUrl": "https://huggingface.co/answerdotai/ModernBERT-large",
      "paperUrl": "?"
    },
    "model_group": "Text Diffusion"
  }
]