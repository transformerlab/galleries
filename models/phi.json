[
    {
        "uniqueID": "microsoft/phi-2",
        "name": "Phi 2",
        "description": "Phi-2 is a Transformer with 2.7 billion parameters. The model hasn't been fine-tuned through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.",
        "parameters": "2B",
        "context": "?k",
        "architecture": "PhiForCausalLM",
        "formats": [
          "Safetensors"
        ],
        "huggingface_repo": "microsoft/phi-2",
        "transformers_version": "4.37.0.dev0",
        "gated": false,
        "license": "MIT",
        "logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTwt9xAK7ih0gG74r3XLTpbiWmcd-9PTwXAQ&usqp=CAU",
        "size_of_model_in_mb": 5305.2,
        "author": {
            "name": "Microsoft",
            "url": "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/microsoft/phi-2",
            "downloadUrl": "https://huggingface.co/microsoft/phi-2",
            "paperUrl": "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/"
        }
    }
]