[
    {
        "uniqueID": "meta-llama/Llama-2-7b-chat-hf",
        "name": "LLama 2 7B - Chat",
        "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format.",
        "parameters": "7B",
        "context": "4k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "meta-llama/Llama-2-7b-chat-hf",
        "transformers_version": "4.31.0.dev0",
        "license": "Meta Custom",
        "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
        "author": {
            "name": "Meta",
            "url": "https://huggingface.co/meta-llama/",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-7b-chat",
            "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-7b-chat",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "meta-llama/Llama-2-7b-hf",
        "name": "LLama 2 7B",
        "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format",
        "parameters": "7B",
        "context": "4k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "meta-llama/Llama-2-7b-hf",
        "transformers_version": "4.31.0.dev0",
        "license": "Meta Custom",
        "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
        "author": {
            "name": "Meta",
            "url": "https://huggingface.co/meta-llama/",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-7b-hf",
            "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-7b-hf",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "meta-llama/Llama-2-13b-hf",
        "name": "LLama 2 13B",
        "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format",
        "parameters": "13B",
        "context": "4k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "meta-llama/Llama-2-13b-hf",
        "transformers_version": "4.31.0.dev0",
        "license": "Meta Custom",
        "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
        "author": {
            "name": "Meta",
            "url": "https://huggingface.co/meta-llama/",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-13b-hf",
            "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-13b-hf",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "meta-llama/Llama-2-13b-chat-hf",
        "name": "LLama 2 13B - Chat",
        "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format",
        "parameters": "7B",
        "context": "4k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "meta-llama/Llama-2-13b-chat-hf",
        "transformers_version": "4.31.0.dev0",
        "license": "Meta Custom",
        "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
        "author": {
            "name": "Meta",
            "url": "https://huggingface.co/meta-llama/",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-13b-chat-hf",
            "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-13b-chat-hf",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "vicuna-7b-v1.5",
        "name": "Vicuna 7b",
        "description": "Vicuna is a state-of-the-art chat assistant developed by LMSYS, leveraging the transformative power of an auto-regressive language model based on the transformer architecture. Trained via fine-tuning the Llama 2 model, Vicuna is used primarily for research in large language models and chatbots, catering to the needs of AI enthusiasts, machine learning researchers, and hobbyists. The model operates under the Llama 2 CLA. The latest version, Vicuna v1.5 (16k), has been fine-tuned using supervised instruction and linear RoPE scaling, with training data comprising around 125K conversations collected from ShareGPT.com, packed into sequences of 16K tokens each. A comprehensive explanation of the training details can be found in the appendix to the linked paper titled \"Training Details of Vicuna Models.\"",
        "parameters": "13B",
        "context": "4k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "lmsys/vicuna-7b-v1.5",
        "transformers_version": "4.31.0",
        "license": "Llama 2 CLA",
        "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
        "author": {
            "name": "LMSYS Org",
            "url": "https://lmsys.org/",
            "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
            "downloadUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
            "paperUrl": "https://arxiv.org/abs/2306.05685"
        }
    },
    {
        "uniqueID": "vicuna-13b-v1.5",
        "name": "Vicuna 13b",
        "description": "Vicuna is a state-of-the-art chat assistant developed by LMSYS, leveraging the transformative power of an auto-regressive language model based on the transformer architecture. Trained via fine-tuning the Llama 2 model, Vicuna is used primarily for research in large language models and chatbots, catering to the needs of AI enthusiasts, machine learning researchers, and hobbyists. The model operates under the Llama 2 CLA. The latest version, Vicuna v1.5 (16k), has been fine-tuned using supervised instruction and linear RoPE scaling, with training data comprising around 125K conversations collected from ShareGPT.com, packed into sequences of 16K tokens each. A comprehensive explanation of the training details can be found in the appendix to the linked paper titled \"Training Details of Vicuna Models.\"",
        "parameters": "13B",
        "context": "4k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "lmsys/vicuna-13b-v1.5",
        "transformers_version": "4.31.0",
        "license": "Llama 2 CLA",
        "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
        "author": {
            "name": "LMSYS Org",
            "url": "https://lmsys.org/",
            "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/lmsys/vicuna-13b-v1.5",
            "downloadUrl": "https://huggingface.co/lmsys/vicuna-13b-v1.5",
            "paperUrl": "https://arxiv.org/abs/2306.05685"
        }
    },
    {
        "uniqueID": "vicuna-7b-v1.5-16k",
        "name": "Vicuna 7b - 16k",
        "description": "Vicuna is a state-of-the-art chat assistant developed by LMSYS, leveraging the transformative power of an auto-regressive language model based on the transformer architecture. Trained via fine-tuning the Llama 2 model, Vicuna is used primarily for research in large language models and chatbots, catering to the needs of AI enthusiasts, machine learning researchers, and hobbyists. The model operates under the Llama 2 CLA. The latest version, Vicuna v1.5 (16k), has been fine-tuned using supervised instruction and linear RoPE scaling, with training data comprising around 125K conversations collected from ShareGPT.com, packed into sequences of 16K tokens each. A comprehensive explanation of the training details can be found in the appendix to the linked paper titled \"Training Details of Vicuna Models.\"",
        "parameters": "7B",
        "context": "16k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "lmsys/vicuna-7b-v1.5-16k",
        "transformers_version": "4.31.0",
        "license": "Llama 2 CLA",
        "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
        "author": {
            "name": "LMSYS Org",
            "url": "https://lmsys.org/",
            "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5-16k",
            "downloadUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5-16k",
            "paperUrl": "https://arxiv.org/abs/2306.05685"
        }
    },
    {
        "uniqueID": "huggyllama/llama-7b",
        "name": "LLama 7B",
        "description": "",
        "parameters": "7B",
        "context": "4k",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "huggyllama/llama-7b",
        "transformers_version": "4.27.4",
        "license": "Meta Custom",
        "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
        "author": {
            "name": "Meta",
            "url": "https://huggingface.co/meta-llama/",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/huggyllama/llama-7b",
            "downloadUrl": "https://huggingface.co/huggyllama/llama-7b",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "Mistral-7B-v0.1",
        "name": "Mistral 7B v0.1",
        "description": "The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. Mistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested.",
        "parameters": "7B",
        "context": "4k",
        "architecture": "MistralForCausalLM",
        "huggingface_repo": "mistralai/Mistral-7B-v0.1",
        "transformers_version": "4.34.0.dev0",
        "license": "Apache 2.0",
        "logo": "https://docs.mistral.ai/img/logo.svg",
        "author": {
            "name": "Mistral AI",
            "url": "https://docs.mistral.ai/",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
            "downloadUrl": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "Mistral-7B-v0.1 Instruct",
        "name": "Mistral-7B-Instruct-v0.1",
        "description": "The Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the Mistral-7B-v0.1 generative text model using a variety of publicly available conversation datasets.",
        "parameters": "7B",
        "context": "4k",
        "architecture": "MistralForCausalLM",
        "huggingface_repo": "mistralai/Mistral-7B-Instruct-v0.1",
        "transformers_version": "4.34.0.dev0",
        "license": "Apache 2.0",
        "logo": "https://docs.mistral.ai/img/logo.svg",
        "author": {
            "name": "Mistral AI",
            "url": "https://docs.mistral.ai/",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
            "downloadUrl": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "tiiuae/falcon-7b",
        "name": "Falcon 7b",
        "description": "Best overall smaller model. Fast responses. Instruction based. Trained by TII. Licensed for commercial use.",
        "parameters": "7B",
        "context": "4k",
        "architecture": "FalconForCausalLM",
        "huggingface_repo": "tiiuae/falcon-7b",
        "transformers_version": "4.27.4",
        "license": "Apache 2.0",
        "logo": "https://falconllm.tii.ae/assets/images/logo.svg",
        "author": {
            "name": "TII",
            "url": "https://falconllm.tii.ae/index.html",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/tiiuae/falcon-7b",
            "downloadUrl": "https://huggingface.co/tiiuae/falcon-7b",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "tiiuae/falcon-7b-instruct",
        "name": "Falcon 7b Instruct",
        "description": "Best overall smaller model. Fast responses. Instruction based. Trained by TII. Licensed for commercial use.",
        "parameters": "7B",
        "context": "4k",
        "architecture": "FalconForCausalLM",
        "huggingface_repo": "tiiuae/falcon-7b-instruct",
        "transformers_version": "4.27.4",
        "license": "Apache 2.0",
        "logo": "https://falconllm.tii.ae/assets/images/logo.svg",
        "author": {
            "name": "TII",
            "url": "https://falconllm.tii.ae/index.html",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/tiiuae/falcon-7b-instruct",
            "downloadUrl": "https://huggingface.co/tiiuae/falcon-7b-instruct",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "google/flan-t5-small",
        "name": "Google Flan T5 Small",
        "description": "",
        "parameters": "80M",
        "context": "2048",
        "architecture": "T5ForConditionalGeneration",
        "huggingface_repo": "google/flan-t5-small",
        "transformers_version": "4.23.1",
        "license": "Apache 2.0",
        "logo": "https://ai.google/static/images/share.png",
        "author": {
            "name": "Google",
            "url": "https://github.com/google-research/FLAN",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/google/flan-t5-small",
            "downloadUrl": "https://huggingface.co/google/flan-t5-small",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "lmsys/fastchat-t5-3b-v1.0",
        "name": "FastChat-T5",
        "description": "Open-source chatbot trained by fine-tuning Flan-t5-xl (3B parameters) on user-shared conversations collected from ShareGPT. It is based on an encoder-decoder transformer architecture, and can autoregressively generate responses to users' inputs.",
        "parameters": "3B",
        "context": "4k",
        "architecture": "T5ForConditionalGeneration",
        "huggingface_repo": "lmsys/fastchat-t5-3b-v1.0",
        "transformers_version": "4.28.1",
        "license": "Apache 2.0",
        "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
        "author": {
            "name": "LMSYS Org",
            "url": "https://lmsys.org/",
            "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/lmsys/fastchat-t5-3b-v1.0",
            "downloadUrl": "https://huggingface.co/lmsys/fastchat-t5-3b-v1.0",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "Nous-Hermes-13b",
        "name": "Nous Hermes 13b",
        "description": "Extremely good model. Instruction based. Gives long responses. Curated with 300,000 uncensored instructions. Trained by Nous Research. Cannot be used commercially",
        "parameters": "13B",
        "context": "?",
        "architecture": "LlamaForCausalLM",
        "huggingface_repo": "NousResearch/Nous-Hermes-13b",
        "transformers_version": "4.29.2",
        "license": "GPL",
        "logo": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/ngyQKdhaykfC8lN6Jfqlz.png?w=200&h=200&f=face",
        "author": {
            "name": "Nous Research",
            "url": "https://nousresearch.com/",
            "blurb": "We are dedicated to advancing the field of natural language processing, in collaboration with the open-source community, through bleeding-edge research and a commitment to symbiotic development."
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
            "downloadUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
            "paperUrl": "?"
        }
    },
    {
        "uniqueID": "HuggingFaceH4/zephyr-7b-alpha",
        "name": "Zephyr 7b Alpha",
        "description": "Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-\u03b1 is the first model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of publicly available, synthetic datasets using Direct Preference Optimization (DPO).",
        "parameters": "7B",
        "context": "4k",
        "architecture": "MistralForCausalLM",
        "huggingface_repo": "HuggingFaceH4/zephyr-7b-alpha",
        "transformers_version": "4.34.0",
        "license": "MIT",
        "logo": "https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png",
        "author": {
            "name": "HuggingFace H4",
            "url": "https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha",
            "blurb": ""
        },
        "resources": {
            "canonicalUrl": "https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha",
            "downloadUrl": "https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha",
            "paperUrl": "?"
        }
    }
]